# -*- coding: utf-8 -*-
import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
import math
import comfy.utils


class ImageRotation:
    """
    å›¾åƒæ—‹è½¬èŠ‚ç‚¹
    åŠŸèƒ½ï¼šå¯¹è¾“å…¥å›¾åƒè¿›è¡Œæ—‹è½¬æ“ä½œï¼Œæ”¯æŒå¤šç§æ—‹è½¬æ¨¡å¼ã€è§’åº¦æ§åˆ¶ã€æ’å€¼ç®—æ³•å’Œå¡«å……é€‰é¡¹
    é€æ˜åº¦å¤„ç†ï¼šå½“"æ˜¯å¦å¡«å……"è®¾ç½®ä¸ºé€æ˜æ—¶ï¼Œè¾“å‡ºå¸¦æœ‰é€æ˜é€šé“çš„RGBAå›¾åƒ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–é¢œè‰²æ˜ å°„"""
        # å®šä¹‰ä¹ç§å¡«å……é¢œè‰² (RGBå€¼ï¼ŒèŒƒå›´0-255)
        self.fill_colors = {
            "é»‘": (0, 0, 0),          # é»‘è‰²
            "ç™½": (255, 255, 255),    # ç™½è‰²
            "èµ¤": (255, 0, 0),        # çº¢è‰²
            "æ©™": (255, 128, 0),      # æ©™è‰²
            "é»„": (255, 255, 0),      # é»„è‰²
            "ç»¿": (0, 255, 0),        # ç»¿è‰²
            "é’": (0, 255, 255),      # é’è‰²
            "è“": (0, 0, 255),        # è“è‰²
            "ç´«": (204, 0, 255),      # ç´«è‰²
        }
        
        # PILæ’å€¼ç®—æ³•æ˜ å°„
        self.resample_methods = {
            "lanczos": Image.Resampling.LANCZOS,
            "bicubic": Image.Resampling.BICUBIC,
            "hamming": Image.Resampling.HAMMING,
            "bilinear": Image.Resampling.BILINEAR,
            "box": Image.Resampling.BOX,
            "nearest": Image.Resampling.NEAREST,
        }
    
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰èŠ‚ç‚¹çš„è¾“å…¥å‚æ•°"""
        return {
            "required": {
                "image": ("IMAGE",),
                "rotation_mode": (["æ­£å‘æ—‹è½¬", "åå‘æ—‹è½¬"], {
                    "default": "æ­£å‘æ—‹è½¬"
                }),
                "rotation_angle": ("INT", {
                    "default": 90,
                    "min": 0,
                    "max": 360,
                    "step": 1,
                    "display": "slider"
                }),
                "interpolation": (["lanczos", "bicubic", "hamming", "bilinear", "box", "nearest"], {
                    "default": "lanczos"
                }),
                "enable_fill": ("BOOLEAN", {
                    "default": False,
                    "label_on": "å¡«å……",
                    "label_off": "é€æ˜"
                }),
                "fill_color": (["é»‘", "ç™½", "èµ¤", "æ©™", "é»„", "ç»¿", "é’", "è“", "ç´«"], {
                    "default": "é»‘"
                }),
            },
        }

    RETURN_TYPES = ("IMAGE", "MASK")
    RETURN_NAMES = ("image", "mask")
    FUNCTION = "rotate_image"
    CATEGORY = "ğŸ¨QING/å›¾åƒå¤„ç†"

    def rotate_image(self, image, rotation_mode, rotation_angle, interpolation, enable_fill, fill_color):
        """
        æ—‹è½¬å›¾åƒ
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒå¼ é‡ [batch, height, width, channels]
            rotation_mode: æ—‹è½¬æ¨¡å¼ ("æ­£å‘æ—‹è½¬" æˆ– "åå‘æ—‹è½¬")
            rotation_angle: æ—‹è½¬è§’åº¦ (0-360)
            interpolation: æ’å€¼ç®—æ³•
            enable_fill: æ˜¯å¦å¯ç”¨å¡«å……
            fill_color: å¡«å……é¢œè‰²
            
        è¿”å›:
            tuple: (æ—‹è½¬åçš„å›¾åƒå¼ é‡, å¡«å……åŒºåŸŸé®ç½©å¼ é‡)
        """
        try:
            # å¦‚æœè§’åº¦ä¸º0æˆ–360ï¼Œç›´æ¥è¿”å›åŸå›¾åƒå’Œå…¨é›¶é®ç½©
            if rotation_angle % 360 == 0:
                # åˆ›å»ºå…¨é›¶é®ç½©ï¼ˆæ— å¡«å……åŒºåŸŸï¼‰
                batch_size, height, width = image.shape[:3]
                zero_mask = torch.zeros((batch_size, height, width), dtype=torch.float32, device=image.device)
                return (image, zero_mask)
            
            # è½¬æ¢è¾“å…¥å¼ é‡ä¸ºPILå›¾åƒåˆ—è¡¨
            batch_size = image.shape[0]
            rotated_images = []
            rotation_masks = []
            
            # ç¡®å®šå®é™…æ—‹è½¬è§’åº¦
            # PILçš„rotate()é»˜è®¤æ˜¯é€†æ—¶é’ˆæ—‹è½¬ï¼Œä¸ºäº†è®©"æ­£å‘æ—‹è½¬"è¡¨ç°ä¸ºé¡ºæ—¶é’ˆï¼Œéœ€è¦å–è´Ÿå€¼
            if rotation_mode == "åå‘æ—‹è½¬":
                actual_angle = rotation_angle  # åå‘æ—‹è½¬ = é€†æ—¶é’ˆ = PILçš„æ­£è§’åº¦
            else:
                actual_angle = -rotation_angle  # æ­£å‘æ—‹è½¬ = é¡ºæ—¶é’ˆ = PILçš„è´Ÿè§’åº¦
            
            # æ ‡å‡†åŒ–è§’åº¦åˆ° -180 åˆ° 180 èŒƒå›´ï¼Œæé«˜æ—‹è½¬ç²¾åº¦
            actual_angle = actual_angle % 360
            if actual_angle > 180:
                actual_angle -= 360
            
            # è·å–æ’å€¼æ–¹æ³•ï¼Œå¤„ç†LANCZOSå…¼å®¹æ€§é—®é¢˜
            resample_method = self.resample_methods.get(interpolation, Image.Resampling.BICUBIC)
            
            # å¯¹äºæŸäº›PILç‰ˆæœ¬ï¼ŒLANCZOSå¯èƒ½ä¸æ”¯æŒæŸäº›æ“ä½œï¼Œfallbackåˆ°BICUBIC
            if interpolation == "lanczos":
                try:
                    # æµ‹è¯•LANCZOSæ˜¯å¦å¯ç”¨
                    test_img = Image.new('RGB', (10, 10))
                    test_img.rotate(1, resample=Image.Resampling.LANCZOS)
                    resample_method = Image.Resampling.LANCZOS
                except:
                    resample_method = Image.Resampling.BICUBIC
            
            # å¤„ç†æ¯å¼ å›¾åƒ
            for i in range(batch_size):
                # å°†å¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ [H, W, C] -> PIL Image
                img_tensor = image[i]  # [H, W, C]
                
                # ç¡®ä¿å¼ é‡åœ¨0-1èŒƒå›´å†…ï¼Œç„¶åè½¬æ¢ä¸º0-255
                if img_tensor.max() <= 1.0:
                    img_array = (img_tensor * 255).clamp(0, 255).cpu().numpy().astype(np.uint8)
                else:
                    img_array = img_tensor.clamp(0, 255).cpu().numpy().astype(np.uint8)
                
                # è½¬æ¢ä¸ºPILå›¾åƒ
                if img_array.shape[2] == 3:  # RGB
                    pil_img = Image.fromarray(img_array, 'RGB')
                elif img_array.shape[2] == 4:  # RGBA
                    pil_img = Image.fromarray(img_array, 'RGBA')
                else:
                    # å•é€šé“è½¬æ¢ä¸ºRGB
                    img_array = np.repeat(img_array, 3, axis=2)
                    pil_img = Image.fromarray(img_array, 'RGB')
                
                # ç”Ÿæˆé®ç½©ï¼šé¦–å…ˆåˆ›å»ºåŸå›¾å¯¹åº”çš„å…¨ç™½é®ç½©
                original_mask = Image.new('L', pil_img.size, 255)  # å…¨ç™½é®ç½©è¡¨ç¤ºåŸå›¾åŒºåŸŸ
                
                # æ—‹è½¬åŸå›¾é®ç½©ä»¥è·å¾—å¡«å……åŒºåŸŸ
                rotated_mask = original_mask.rotate(
                    actual_angle,
                    resample=resample_method,
                    expand=True,
                    fillcolor=0  # å¡«å……åŒºåŸŸç”¨é»‘è‰²(0)è¡¨ç¤º
                )
                
                # æ‰§è¡Œæ—‹è½¬
                if enable_fill:
                    # ä½¿ç”¨æŒ‡å®šé¢œè‰²å¡«å……
                    fill_rgb = self.fill_colors[fill_color]
                    # å¦‚æœåŸå›¾æœ‰alphaé€šé“ï¼Œæ·»åŠ alpha=255
                    if pil_img.mode == 'RGBA':
                        fillcolor = fill_rgb + (255,)
                    else:
                        fillcolor = fill_rgb
                    
                    rotated_pil = pil_img.rotate(
                        actual_angle, 
                        resample=resample_method,
                        expand=True,
                        fillcolor=fillcolor
                    )
                else:
                    # ä½¿ç”¨é€æ˜å¡«å……ï¼ˆalphaé€šé“ï¼‰
                    if pil_img.mode != 'RGBA':
                        pil_img = pil_img.convert('RGBA')
                    
                    rotated_pil = pil_img.rotate(
                        actual_angle, 
                        resample=resample_method,
                        expand=True,
                        fillcolor=(0, 0, 0, 0)  # é€æ˜å¡«å……
                    )
                
                # å¤„ç†è¾“å‡ºæ ¼å¼ï¼ˆä¿æŒé€æ˜åº¦æˆ–è½¬æ¢ä¸ºRGBï¼‰
                if rotated_pil.mode == 'RGBA':
                    if not enable_fill:
                        # ä¿æŒRGBAæ ¼å¼ä»¥ä¿ç•™é€æ˜åº¦ä¿¡æ¯
                        rotated_array = np.array(rotated_pil).astype(np.float32) / 255.0
                        rotated_tensor = torch.from_numpy(rotated_array)
                    else:
                        # å¡«å……æ¨¡å¼ï¼šè½¬æ¢ä¸ºRGB
                        rotated_pil = rotated_pil.convert('RGB')
                        rotated_array = np.array(rotated_pil).astype(np.float32) / 255.0
                        rotated_tensor = torch.from_numpy(rotated_array)
                else:
                    # ç¡®ä¿ä¸ºRGBæ ¼å¼
                    if rotated_pil.mode != 'RGB':
                        rotated_pil = rotated_pil.convert('RGB')
                    rotated_array = np.array(rotated_pil).astype(np.float32) / 255.0
                    rotated_tensor = torch.from_numpy(rotated_array)
                
                # å¤„ç†é®ç½©ï¼šå°†é®ç½©è½¬æ¢ä¸ºå¼ é‡
                # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦åè½¬é®ç½©ï¼Œä½¿å¡«å……åŒºåŸŸä¸ºç™½è‰²(1.0)ï¼ŒåŸå›¾åŒºåŸŸä¸ºé»‘è‰²(0.0)
                mask_array = np.array(rotated_mask).astype(np.float32) / 255.0
                # åè½¬é®ç½©ï¼šåŸå›¾åŒºåŸŸ(ç™½è‰²255->1.0)å˜ä¸º0.0ï¼Œå¡«å……åŒºåŸŸ(é»‘è‰²0->0.0)å˜ä¸º1.0
                mask_array = 1.0 - mask_array
                mask_tensor = torch.from_numpy(mask_array)
                
                # ç¡®ä¿é®ç½©ä¸å›¾åƒåœ¨åŒä¸€è®¾å¤‡ä¸Š
                mask_tensor = mask_tensor.to(device=image.device)
                
                
                rotated_images.append(rotated_tensor)
                rotation_masks.append(mask_tensor)
            
            # åˆå¹¶æ‰¹æ¬¡
            result_tensor = torch.stack(rotated_images, dim=0)
            mask_tensor = torch.stack(rotation_masks, dim=0)
            
            return (result_tensor, mask_tensor)
            
        except Exception as e:
            print(f"å›¾åƒæ—‹è½¬é”™è¯¯: {e}")
            # è¿”å›åŸå›¾åƒå’Œå…¨é›¶é®ç½©ä½œä¸ºfallback
            batch_size, height, width = image.shape[:3]
            fallback_mask = torch.zeros((batch_size, height, width), dtype=torch.float32, device=image.device)
            return (image, fallback_mask)


class ImageFlipping:
    """
    å›¾åƒç¿»è½¬èŠ‚ç‚¹
    åŠŸèƒ½ï¼šå¯¹è¾“å…¥å›¾åƒè¿›è¡Œç¿»è½¬æ“ä½œï¼Œæ”¯æŒæ°´å¹³ç¿»è½¬å’Œå‚ç›´ç¿»è½¬ï¼Œä»¥åŠå¤šç§æ’å€¼ç®—æ³•
    é€æ˜åº¦å¤„ç†ï¼šè‡ªåŠ¨ä¿æŒåŸå§‹å›¾åƒçš„é€æ˜åº¦ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ’å€¼ç®—æ³•æ˜ å°„"""
        # PILæ’å€¼ç®—æ³•æ˜ å°„
        self.resample_methods = {
            "lanczos": Image.Resampling.LANCZOS,
            "bicubic": Image.Resampling.BICUBIC,
            "hamming": Image.Resampling.HAMMING,
            "bilinear": Image.Resampling.BILINEAR,
            "box": Image.Resampling.BOX,
            "nearest": Image.Resampling.NEAREST,
        }
    
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰èŠ‚ç‚¹çš„è¾“å…¥å‚æ•°"""
        return {
            "required": {
                "image": ("IMAGE",),
                "flip_mode": (["æ°´å¹³ç¿»è½¬", "å‚ç›´ç¿»è½¬"], {
                    "default": "æ°´å¹³ç¿»è½¬"
                }),
                "interpolation": (["lanczos", "bicubic", "hamming", "bilinear", "box", "nearest"], {
                    "default": "lanczos"
                }),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)
    FUNCTION = "flip_image"
    CATEGORY = "ğŸ¨QING/å›¾åƒå¤„ç†"

    def flip_image(self, image, flip_mode, interpolation):
        """
        ç¿»è½¬å›¾åƒ
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒå¼ é‡ [batch, height, width, channels]
            flip_mode: ç¿»è½¬æ¨¡å¼ ("æ°´å¹³ç¿»è½¬" æˆ– "å‚ç›´ç¿»è½¬")
            interpolation: æ’å€¼ç®—æ³•
            
        è¿”å›:
            tuple: (ç¿»è½¬åçš„å›¾åƒå¼ é‡,)
        """
        try:
            # è½¬æ¢è¾“å…¥å¼ é‡ä¸ºPILå›¾åƒåˆ—è¡¨
            batch_size = image.shape[0]
            flipped_images = []
            
            # è·å–æ’å€¼æ–¹æ³•ï¼ˆæ³¨æ„ï¼šç¿»è½¬æ“ä½œæœ¬èº«ä¸éœ€è¦æ’å€¼ï¼Œä½†ä¸ºäº†ä¿æŒä¸€è‡´æ€§ä¿ç•™æ­¤å‚æ•°ï¼‰
            resample_method = self.resample_methods.get(interpolation, Image.Resampling.LANCZOS)
            
            # å¤„ç†æ¯å¼ å›¾åƒ
            for i in range(batch_size):
                # å°†å¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ [H, W, C] -> PIL Image
                img_tensor = image[i]  # [H, W, C]
                
                # ç¡®ä¿å¼ é‡åœ¨0-1èŒƒå›´å†…ï¼Œç„¶åè½¬æ¢ä¸º0-255
                if img_tensor.max() <= 1.0:
                    img_array = (img_tensor * 255).clamp(0, 255).cpu().numpy().astype(np.uint8)
                else:
                    img_array = img_tensor.clamp(0, 255).cpu().numpy().astype(np.uint8)
                
                # è½¬æ¢ä¸ºPILå›¾åƒ
                if img_array.shape[2] == 3:  # RGB
                    pil_img = Image.fromarray(img_array, 'RGB')
                elif img_array.shape[2] == 4:  # RGBA
                    pil_img = Image.fromarray(img_array, 'RGBA')
                else:
                    # å•é€šé“è½¬æ¢ä¸ºRGB
                    img_array = np.repeat(img_array, 3, axis=2)
                    pil_img = Image.fromarray(img_array, 'RGB')
                
                # æ‰§è¡Œç¿»è½¬
                if flip_mode == "æ°´å¹³ç¿»è½¬":
                    flipped_pil = pil_img.transpose(Image.Transpose.FLIP_LEFT_RIGHT)
                elif flip_mode == "å‚ç›´ç¿»è½¬":
                    flipped_pil = pil_img.transpose(Image.Transpose.FLIP_TOP_BOTTOM)
                else:
                    # é»˜è®¤æ°´å¹³ç¿»è½¬
                    flipped_pil = pil_img.transpose(Image.Transpose.FLIP_LEFT_RIGHT)
                
                # å¤„ç†è¾“å‡ºæ ¼å¼ï¼ˆä¿æŒåŸå§‹æ ¼å¼çš„é€æ˜åº¦ä¿¡æ¯ï¼‰
                if flipped_pil.mode == 'RGBA':
                    # ä¿æŒRGBAæ ¼å¼ä»¥ä¿ç•™é€æ˜åº¦ä¿¡æ¯
                    flipped_array = np.array(flipped_pil).astype(np.float32) / 255.0
                    flipped_tensor = torch.from_numpy(flipped_array)
                else:
                    # ç¡®ä¿ä¸ºRGBæ ¼å¼
                    if flipped_pil.mode != 'RGB':
                        flipped_pil = flipped_pil.convert('RGB')
                    flipped_array = np.array(flipped_pil).astype(np.float32) / 255.0
                    flipped_tensor = torch.from_numpy(flipped_array)
                
                flipped_images.append(flipped_tensor)
            
            # åˆå¹¶æ‰¹æ¬¡
            result_tensor = torch.stack(flipped_images, dim=0)
            
            return (result_tensor,)
            
        except Exception as e:
            print(f"å›¾åƒç¿»è½¬é”™è¯¯: {e}")
            # è¿”å›åŸå›¾åƒä½œä¸ºfallback
            return (image,)


class ImageScaling:
    """
    å›¾åƒç¼©æ”¾èŠ‚ç‚¹ - å®ç°å›¾åƒå’Œé®ç½©çš„é«˜çº§ç¼©æ”¾åŠŸèƒ½
    
    åŠŸèƒ½:
    - åŒæ—¶å¤„ç†å›¾åƒå’Œé®ç½©çš„ç¼©æ”¾
    - æ”¯æŒå¤šç§ç¼©æ”¾æ–¹å¼ï¼šæ‹‰ä¼¸ã€è£å‰ªã€å¡«å……ã€ä¿æŒæ¯”ä¾‹
    - æ”¯æŒå¤šç§æ’å€¼æ–¹æ³•ï¼šnearestã€bilinearã€bicubicã€areaã€nearest-exactã€lanczos
    - æ”¯æŒå¤šç§ç¼©æ”¾å®šä¹‰ï¼šæ— ã€æœ€é•¿è¾¹ã€æœ€çŸ­è¾¹ã€å®½åº¦ã€é«˜åº¦ã€ç™¾åˆ†æ¯”ã€æ€»åƒç´ 
    - æ”¯æŒå›¾åƒå€æ•°çº¦æŸ
    - è¾“å‡ºæœ€ç»ˆçš„å›¾åƒã€é®ç½©å’Œå°ºå¯¸ä¿¡æ¯
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "scale_mode": (["keep_ratio", "stretch", "crop", "pad"], {
                    "default": "keep_ratio"
                }),
                "interpolation": (["lanczos", "nearest", "bilinear", "bicubic", "area", "nearest-exact"], {
                    "default": "lanczos"
                }),
                "scale_definition": (["none", "longest_side", "shortest_side", "width", "height", "percentage", "total_pixels"], {
                    "default": "longest_side"
                }),
                "definition_value": ("INT", {
                    "default": 1024,
                    "min": 1,
                    "max": 999999999,
                    "step": 1
                }),
                "multiple_of": ("INT", {
                    "default": 2,
                    "min": 1,
                    "max": 256,
                    "step": 1
                }),
            },
            "optional": {
                "image": ("IMAGE",),
                "mask": ("MASK",),
                "width": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 999999999,
                    "step": 1
                }),
                "height": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 999999999,
                    "step": 1
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "MASK", "INT", "INT")
    RETURN_NAMES = ("image", "mask", "width", "height")
    FUNCTION = "scale_image"
    CATEGORY = "ğŸ¨QING/å›¾åƒå¤„ç†"
    
    def scale_image(self, scale_mode, interpolation, scale_definition, definition_value, multiple_of,
                   width=0, height=0, image=None, mask=None):
        """
        ä¸»å¤„ç†å‡½æ•°ï¼šå®ç°å›¾åƒå’Œé®ç½©çš„ç¼©æ”¾
        
        å‚æ•°:
            scale_mode: ç¼©æ”¾æ–¹å¼
            interpolation: æ’å€¼æ–¹æ³•
            scale_definition: ç¼©æ”¾å®šä¹‰
            definition_value: å®šä¹‰è°ƒæ•´å€¼
            multiple_of: å›¾åƒå€æ•°
            image: è¾“å…¥å›¾åƒï¼ˆå¯é€‰ï¼‰
            mask: è¾“å…¥é®ç½©ï¼ˆå¯é€‰ï¼‰
            width: ç›®æ ‡å®½åº¦ï¼ˆå¯é€‰ï¼‰
            height: ç›®æ ‡é«˜åº¦ï¼ˆå¯é€‰ï¼‰
            
        è¿”å›:
            tuple: (ç¼©æ”¾åçš„å›¾åƒ, ç¼©æ”¾åçš„é®ç½©, æœ€ç»ˆå®½åº¦, æœ€ç»ˆé«˜åº¦)
        """
        
        # éªŒè¯è¾“å…¥
        if image is None and mask is None:
            raise ValueError("è‡³å°‘éœ€è¦æä¾›å›¾åƒæˆ–é®ç½©ä¸­çš„ä¸€ä¸ª")
        
        # è·å–åŸå§‹å°ºå¯¸
        if image is not None:
            orig_height, orig_width = self._get_image_size(image)
        elif mask is not None:
            orig_height, orig_width = self._get_mask_size(mask)
        else:
            orig_height, orig_width = 512, 512  # é»˜è®¤å°ºå¯¸
        
        # è®¡ç®—ç›®æ ‡å°ºå¯¸
        target_width, target_height = self._calculate_target_size(
            orig_width, orig_height, width, height, scale_mode, 
            scale_definition, definition_value, multiple_of
        )
        
        # ç¼©æ”¾å›¾åƒ
        scaled_image = None
        if image is not None:
            scaled_image = self._scale_image_tensor(
                image, target_width, target_height, scale_mode, interpolation
            )
        else:
            # åˆ›å»ºç©ºå›¾åƒ
            scaled_image = self._create_empty_image(target_width, target_height)
        
        # ç¼©æ”¾é®ç½©
        scaled_mask = None
        if mask is not None:
            scaled_mask = self._scale_mask_tensor(
                mask, target_width, target_height, interpolation
            )
        else:
            # åˆ›å»ºç™½è‰²é®ç½©
            scaled_mask = self._create_white_mask(target_width, target_height)
        
        return (scaled_image, scaled_mask, target_width, target_height)
    
    def _get_image_size(self, image):
        """è·å–å›¾åƒå°ºå¯¸"""
        if image.dim() == 4:  # (B, H, W, C)
            return image.shape[1], image.shape[2]
        elif image.dim() == 3:  # (H, W, C)
            return image.shape[0], image.shape[1]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒç»´åº¦: {image.dim()}")
    
    def _get_mask_size(self, mask):
        """è·å–é®ç½©å°ºå¯¸"""
        if mask.dim() == 3:  # (B, H, W)
            return mask.shape[1], mask.shape[2]
        elif mask.dim() == 2:  # (H, W)
            return mask.shape[0], mask.shape[1]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é®ç½©ç»´åº¦: {mask.dim()}")
    
    def _calculate_target_size(self, orig_width, orig_height, width, height, 
                              scale_mode, scale_definition, definition_value, multiple_of):
        """è®¡ç®—ç›®æ ‡å°ºå¯¸"""
        
        # å¦‚æœç›´æ¥æŒ‡å®šäº†å®½é«˜ï¼Œä¼˜å…ˆä½¿ç”¨
        if width > 0 and height > 0:
            target_width, target_height = width, height
        elif width > 0:
            target_width = width
            if scale_mode == "pad":
                # padæ¨¡å¼ï¼šå¦‚æœåªæŒ‡å®šå®½åº¦ï¼Œé«˜åº¦ä¿æŒåŸå§‹å€¼ï¼ˆä¸ç¼©æ”¾ï¼‰
                target_height = orig_height
            elif scale_mode == "keep_ratio":
                target_height = max(1, int(orig_height * (width / orig_width)))
            else:
                target_height = orig_height
        elif height > 0:
            target_height = height
            if scale_mode == "pad":
                # padæ¨¡å¼ï¼šå¦‚æœåªæŒ‡å®šé«˜åº¦ï¼Œå®½åº¦ä¿æŒåŸå§‹å€¼ï¼ˆä¸ç¼©æ”¾ï¼‰
                target_width = orig_width
            elif scale_mode == "keep_ratio":
                target_width = max(1, int(orig_width * (height / orig_height)))
            else:
                target_width = orig_width
        else:
            # æ ¹æ®ç¼©æ”¾å®šä¹‰è®¡ç®—å°ºå¯¸
            target_width, target_height = self._calculate_by_definition(
                orig_width, orig_height, scale_definition, definition_value, scale_mode
            )
        
        # åº”ç”¨å€æ•°çº¦æŸ
        if multiple_of > 1:
            # ä½¿ç”¨å°±è¿‘èˆå…¥æ¨¡å¼ï¼Œå‡å°‘ä¸å¿…è¦çš„å°ºå¯¸å˜åŒ–
            target_width = round(target_width / multiple_of) * multiple_of
            target_height = round(target_height / multiple_of) * multiple_of
        
        # ç¡®ä¿æœ€å°å°ºå¯¸
        target_width = max(1, target_width)
        target_height = max(1, target_height)
        
        return target_width, target_height
    
    def _calculate_by_definition(self, orig_width, orig_height, scale_definition, 
                                definition_value, scale_mode):
        """æ ¹æ®ç¼©æ”¾å®šä¹‰è®¡ç®—ç›®æ ‡å°ºå¯¸"""
        
        if scale_definition == "none":
            return orig_width, orig_height
        
        elif scale_definition == "longest_side":
            if orig_width >= orig_height:
                target_width = definition_value
                if scale_mode in ["keep_ratio", "pad"]:
                    target_height = max(1, int(orig_height * (definition_value / orig_width)))
                else:
                    target_height = orig_height
            else:
                target_height = definition_value
                if scale_mode in ["keep_ratio", "pad"]:
                    target_width = max(1, int(orig_width * (definition_value / orig_height)))
                else:
                    target_width = orig_width
        
        elif scale_definition == "shortest_side":
            if orig_width <= orig_height:
                target_width = definition_value
                if scale_mode in ["keep_ratio", "pad"]:
                    target_height = max(1, int(orig_height * (definition_value / orig_width)))
                else:
                    target_height = orig_height
            else:
                target_height = definition_value
                if scale_mode in ["keep_ratio", "pad"]:
                    target_width = max(1, int(orig_width * (definition_value / orig_height)))
                else:
                    target_width = orig_width
        
        elif scale_definition == "width":
            target_width = definition_value
            if scale_mode in ["keep_ratio", "pad"]:
                target_height = max(1, int(orig_height * (definition_value / orig_width)))
            else:
                target_height = orig_height
        
        elif scale_definition == "height":
            target_height = definition_value
            if scale_mode in ["keep_ratio", "pad"]:
                target_width = max(1, int(orig_width * (definition_value / orig_height)))
            else:
                target_width = orig_width
        
        elif scale_definition == "percentage":
            scale_factor = definition_value / 100.0
            target_width = max(1, int(orig_width * scale_factor))
            target_height = max(1, int(orig_height * scale_factor))
        
        elif scale_definition == "total_pixels":
            if scale_mode in ["keep_ratio", "pad"]:
                aspect_ratio = orig_width / orig_height
                target_height = max(1, int(math.sqrt(definition_value / aspect_ratio)))
                target_width = max(1, int(target_height * aspect_ratio))
            else:
                # éä¿æŒæ¯”ä¾‹æ—¶ï¼Œå¹³å‡åˆ†é…åƒç´ 
                side_length = max(1, int(math.sqrt(definition_value)))
                target_width = side_length
                target_height = side_length
        
        else:
            target_width, target_height = orig_width, orig_height
        
        return target_width, target_height
    
    def _scale_image_tensor(self, image, target_width, target_height, scale_mode, interpolation):
        """ç¼©æ”¾å›¾åƒå¼ é‡"""
        
        # ç¡®ä¿å›¾åƒæ˜¯4ç»´ (B, H, W, C)
        if image.dim() == 3:
            image = image.unsqueeze(0)
        
        device = image.device
        dtype = image.dtype
        
        # è·å–åŸå§‹å°ºå¯¸
        batch_size, orig_height, orig_width, channels = image.shape
        
        if scale_mode == "stretch":
            # ç›´æ¥æ‹‰ä¼¸åˆ°ç›®æ ‡å°ºå¯¸
            return self._resize_image(image, target_width, target_height, interpolation)
        
        elif scale_mode == "keep_ratio":
            # ä¿æŒæ¯”ä¾‹ç¼©æ”¾ï¼Œå¯èƒ½ä¼šæœ‰é»‘è¾¹
            scale_x = target_width / orig_width
            scale_y = target_height / orig_height
            scale = min(scale_x, scale_y)
            
            new_width = int(orig_width * scale)
            new_height = int(orig_height * scale)
            
            # å…ˆç¼©æ”¾åˆ°åˆé€‚å¤§å°
            scaled = self._resize_image(image, new_width, new_height, interpolation)
            
            # ç„¶åå¡«å……åˆ°ç›®æ ‡å°ºå¯¸
            return self._pad_image(scaled, target_width, target_height)
        
        elif scale_mode == "crop":
            # ä¿æŒæ¯”ä¾‹ç¼©æ”¾ï¼Œè£å‰ªå¤šä½™éƒ¨åˆ†
            scale_x = target_width / orig_width
            scale_y = target_height / orig_height
            scale = max(scale_x, scale_y)
            
            new_width = int(orig_width * scale)
            new_height = int(orig_height * scale)
            
            # å…ˆç¼©æ”¾åˆ°åˆé€‚å¤§å°
            scaled = self._resize_image(image, new_width, new_height, interpolation)
            
            # ç„¶åå±…ä¸­è£å‰ª
            return self._center_crop_image(scaled, target_width, target_height)
        
        elif scale_mode == "pad":
            # padæ¨¡å¼ï¼šä¿æŒåŸå§‹å°ºå¯¸ï¼Œç›´æ¥å¡«å……åˆ°ç›®æ ‡å°ºå¯¸
            # ä¸è¿›è¡Œä»»ä½•ç¼©æ”¾ï¼Œç›´æ¥å°†åŸå›¾æ”¾ç½®åœ¨ç›®æ ‡ç”»å¸ƒä¸­å¿ƒ
            return self._pad_image(image, target_width, target_height)
        
        else:
            return self._resize_image(image, target_width, target_height, interpolation)
    
    def _scale_mask_tensor(self, mask, target_width, target_height, interpolation):
        """ç¼©æ”¾é®ç½©å¼ é‡"""
        
        # ç¡®ä¿é®ç½©æ˜¯3ç»´ (B, H, W)
        if mask.dim() == 2:
            mask = mask.unsqueeze(0)
        
        device = mask.device
        dtype = mask.dtype
        
        # é®ç½©å§‹ç»ˆä½¿ç”¨æœ€è¿‘é‚»æˆ–åŒçº¿æ€§æ’å€¼ï¼Œä¸ä½¿ç”¨å¤æ‚çš„æ’å€¼æ–¹æ³•
        if interpolation in ["nearest", "nearest-exact"]:
            mode = "nearest"
        else:
            mode = "bilinear"
        
        # è½¬æ¢ä¸ºfloatè¿›è¡Œæ’å€¼
        mask_float = mask.float().unsqueeze(1)  # (B, 1, H, W)
        
        # ä½¿ç”¨F.interpolateè¿›è¡Œç¼©æ”¾
        scaled_mask = F.interpolate(
            mask_float,
            size=(target_height, target_width),
            mode=mode,
            align_corners=False if mode != "nearest" else None
        ).squeeze(1)  # (B, H, W)
        
        # è½¬å›åŸå§‹æ•°æ®ç±»å‹å¹¶é™åˆ¶èŒƒå›´
        scaled_mask = torch.clamp(scaled_mask, 0.0, 1.0)
        if dtype != torch.float32:
            scaled_mask = scaled_mask.to(dtype)
        
        return scaled_mask
    
    def _resize_image(self, image, target_width, target_height, interpolation):
        """ä½¿ç”¨æŒ‡å®šæ’å€¼æ–¹æ³•ç¼©æ”¾å›¾åƒ"""
        
        device = image.device
        dtype = image.dtype
        
        if interpolation == "lanczos":
            # ä½¿ç”¨PILè¿›è¡ŒLanczosæ’å€¼
            return self._resize_image_pil(image, target_width, target_height, Image.LANCZOS)
        
        elif interpolation == "area":
            # ä½¿ç”¨PILè¿›è¡ŒåŒºåŸŸæ’å€¼ï¼ˆé€‚åˆç¼©å°ï¼‰
            return self._resize_image_pil(image, target_width, target_height, Image.BOX)
        
        elif interpolation == "nearest-exact":
            # ç²¾ç¡®æœ€è¿‘é‚»æ’å€¼
            return self._resize_image_pil(image, target_width, target_height, Image.NEAREST)
        
        else:
            # ä½¿ç”¨PyTorchçš„F.interpolate
            mode_map = {
                "nearest": "nearest",
                "bilinear": "bilinear",
                "bicubic": "bicubic"
            }
            mode = mode_map.get(interpolation, "bilinear")
            
            # è½¬æ¢ç»´åº¦ (B, H, W, C) -> (B, C, H, W)
            image_transposed = image.permute(0, 3, 1, 2)
            
            # ç¼©æ”¾
            scaled = F.interpolate(
                image_transposed,
                size=(target_height, target_width),
                mode=mode,
                align_corners=False if mode != "nearest" else None
            )
            
            # è½¬æ¢å› (B, H, W, C)
            return scaled.permute(0, 2, 3, 1)
    
    def _resize_image_pil(self, image, target_width, target_height, pil_method):
        """ä½¿ç”¨PILè¿›è¡Œå›¾åƒç¼©æ”¾"""
        
        device = image.device
        dtype = image.dtype
        batch_size = image.shape[0]
        
        scaled_images = []
        for i in range(batch_size):
            # è½¬æ¢ä¸ºPILå›¾åƒ
            img_np = (image[i].cpu().numpy() * 255).astype(np.uint8)
            if img_np.shape[2] == 1:
                pil_img = Image.fromarray(img_np[:, :, 0], mode='L')
            elif img_np.shape[2] == 3:
                pil_img = Image.fromarray(img_np, mode='RGB')
            elif img_np.shape[2] == 4:
                pil_img = Image.fromarray(img_np, mode='RGBA')
            else:
                # ä¸æ”¯æŒçš„é€šé“æ•°ï¼Œå–å‰3ä¸ªé€šé“
                pil_img = Image.fromarray(img_np[:, :, :3], mode='RGB')
            
            # ç¼©æ”¾
            resized_pil = pil_img.resize((target_width, target_height), pil_method)
            
            # è½¬æ¢å›å¼ é‡
            resized_np = np.array(resized_pil).astype(np.float32) / 255.0
            if resized_np.ndim == 2:
                resized_np = resized_np[:, :, np.newaxis]  # æ·»åŠ é€šé“ç»´åº¦
            elif resized_np.ndim == 3 and resized_np.shape[2] != image.shape[3]:
                # è°ƒæ•´é€šé“æ•°ä»¥åŒ¹é…åŸå§‹å›¾åƒ
                if image.shape[3] == 3 and resized_np.shape[2] == 4:
                    resized_np = resized_np[:, :, :3]  # ç§»é™¤alphaé€šé“
                elif image.shape[3] == 4 and resized_np.shape[2] == 3:
                    # æ·»åŠ alphaé€šé“
                    alpha = np.ones((resized_np.shape[0], resized_np.shape[1], 1), dtype=np.float32)
                    resized_np = np.concatenate([resized_np, alpha], axis=2)
                elif image.shape[3] == 1:
                    resized_np = resized_np.mean(axis=2, keepdims=True)  # è½¬ä¸ºç°åº¦
            
            resized_tensor = torch.from_numpy(resized_np).to(device=device, dtype=dtype)
            scaled_images.append(resized_tensor)
        
        return torch.stack(scaled_images)
    
    def _pad_image(self, image, target_width, target_height):
        """å¡«å……å›¾åƒåˆ°ç›®æ ‡å°ºå¯¸"""
        
        batch_size, orig_height, orig_width, channels = image.shape
        
        if orig_width == target_width and orig_height == target_height:
            return image
        
        device = image.device
        dtype = image.dtype
        
        # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„é»‘è‰²å›¾åƒ
        padded = torch.zeros((batch_size, target_height, target_width, channels), 
                           device=device, dtype=dtype)
        
        # è®¡ç®—å±…ä¸­ä½ç½®å’Œè£å‰ª/å¡«å……å‚æ•°
        if orig_width <= target_width and orig_height <= target_height:
            # åŸå›¾å°äºç›®æ ‡å°ºå¯¸ï¼Œéœ€è¦å¡«å……
            start_x = (target_width - orig_width) // 2
            start_y = (target_height - orig_height) // 2
            
            # å¤åˆ¶æ•´ä¸ªåŸå›¾åˆ°ç›®æ ‡ä½ç½®
            padded[:, start_y:start_y+orig_height, start_x:start_x+orig_width, :] = image
        else:
            # åŸå›¾å¤§äºç›®æ ‡å°ºå¯¸ï¼Œéœ€è¦å±…ä¸­è£å‰ªç„¶åæ”¾ç½®
            # è®¡ç®—åŸå›¾çš„è£å‰ªåŒºåŸŸ
            crop_start_x = max(0, (orig_width - target_width) // 2)
            crop_start_y = max(0, (orig_height - target_height) // 2)
            
            # è®¡ç®—å®é™…å¤åˆ¶çš„å°ºå¯¸
            copy_width = min(orig_width, target_width)
            copy_height = min(orig_height, target_height)
            
            # è®¡ç®—åœ¨ç›®æ ‡å›¾åƒä¸­çš„æ”¾ç½®ä½ç½®
            pad_start_x = max(0, (target_width - copy_width) // 2)
            pad_start_y = max(0, (target_height - copy_height) // 2)
            
            # ä»åŸå›¾è£å‰ªå¹¶å¤åˆ¶åˆ°ç›®æ ‡ä½ç½®
            padded[:, pad_start_y:pad_start_y+copy_height, pad_start_x:pad_start_x+copy_width, :] = \
                image[:, crop_start_y:crop_start_y+copy_height, crop_start_x:crop_start_x+copy_width, :]
        
        return padded
    
    def _center_crop_image(self, image, target_width, target_height):
        """å±…ä¸­è£å‰ªå›¾åƒ"""
        
        batch_size, orig_height, orig_width, channels = image.shape
        
        if orig_width == target_width and orig_height == target_height:
            return image
        
        # è®¡ç®—è£å‰ªèµ·å§‹ä½ç½®
        start_x = (orig_width - target_width) // 2
        start_y = (orig_height - target_height) // 2
        
        # ç¡®ä¿ä¸ä¼šè¶Šç•Œ
        start_x = max(0, start_x)
        start_y = max(0, start_y)
        end_x = min(start_x + target_width, orig_width)
        end_y = min(start_y + target_height, orig_height)
        
        # è£å‰ªå›¾åƒ
        cropped = image[:, start_y:end_y, start_x:end_x, :]
        
        # å¦‚æœè£å‰ªåå°ºå¯¸ä¸è¶³ï¼Œéœ€è¦å¡«å……
        actual_height, actual_width = cropped.shape[1], cropped.shape[2]
        if actual_width < target_width or actual_height < target_height:
            cropped = self._pad_image(cropped, target_width, target_height)
        
        return cropped
    
    def _create_empty_image(self, width, height):
        """åˆ›å»ºç©ºçš„é»‘è‰²å›¾åƒ"""
        return torch.zeros((1, height, width, 3), dtype=torch.float32)
    
    def _create_white_mask(self, width, height):
        """åˆ›å»ºç™½è‰²é®ç½©"""
        return torch.ones((1, height, width), dtype=torch.float32)


class MaskScale:
    """
    é®ç½©ç¼©æ”¾èŠ‚ç‚¹ - ä¸“é—¨å¤„ç†é®ç½©çš„ç¼©æ”¾åŠŸèƒ½
    
    åŠŸèƒ½:
    - ä¸“æ³¨äºé®ç½©çš„é«˜è´¨é‡ç¼©æ”¾
    - æ”¯æŒå¤šç§ç¼©æ”¾å®šä¹‰æ–¹å¼
    - æ”¯æŒä¿æŒæ¯”ä¾‹æˆ–è‡ªç”±ç¼©æ”¾
    - é«˜æ•ˆçš„æ’å€¼ç®—æ³•
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "mask": ("MASK",),
                "scale_definition": (["width", "height", "longest_side", "shortest_side", "total_pixels"], {
                    "default": "longest_side"
                }),
                "definition_value": ("INT", {
                    "default": 512,
                    "min": 1,
                    "max": 999999999,
                    "step": 1
                }),
                "interpolation": (["nearest", "bilinear", "bicubic", "lanczos"], {
                    "default": "lanczos"
                }),
                "keep_proportions": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "width": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 999999999,
                    "step": 1
                }),
                "height": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 999999999,
                    "step": 1
                }),
            }
        }
    
    # è¿”å›ç±»å‹ä¸åç§°ï¼ˆç»Ÿä¸€ä¸­æ–‡æ˜¾ç¤ºï¼‰
    RETURN_TYPES = ("MASK", "INT", "INT")
    RETURN_NAMES = ("mask", "width", "height")
    FUNCTION = "scale_mask"
    CATEGORY = "ğŸ¨QING/é®ç½©å¤„ç†"
    
    def scale_mask(self, mask, scale_definition, definition_value, interpolation, keep_proportions, width=0, height=0):
        """
        ç¼©æ”¾é®ç½©å°ºå¯¸
        - é®ç½©: è¾“å…¥é®ç½© (Tensor: [B,H,W])
        - scale_definition: ç¼©æ”¾å®šä¹‰ï¼šwidth/height/longest_side/shortest_side/total_pixels
        - definition_value: å®šä¹‰æ•°å€¼ï¼ˆä¸ scale_definition å¯¹åº”ï¼‰
        - interpolation: interpolationï¼šnearest/bilinear/bicubic/lanczos
        - keep_proportions: æ˜¯å¦ä¿æŒçºµæ¨ªæ¯”
        - width/height: æŒ‡å®šç›®æ ‡å°ºå¯¸ï¼ˆä¼˜å…ˆäº scale_definitionï¼‰
        è¿”å›ï¼šç¼©æ”¾åçš„é®ç½©ï¼Œä»¥åŠç›®æ ‡å®½é«˜
        """
        # è¾“å…¥æ ¡éªŒ
        if mask is None:
            raise ValueError("Input mask cannot be None")
        
        # Ensure input is 2D or 3D tensor
        if mask.dim() == 2:
            mask = mask.unsqueeze(0)
        elif mask.dim() != 3:
            raise ValueError(f"Mask dimension should be 2 or 3, got {mask.dim()}")
        
        # è·å–å½“å‰é®ç½©å°ºå¯¸
        batch_size, orig_height, orig_width = mask.shape
        
        # è‹¥æ˜¾å¼æä¾›ç›®æ ‡å°ºå¯¸ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨
        if width > 0 and height > 0:
            target_width = max(1, width)
            target_height = max(1, height)
        else:
            # æ ¹æ®ç¼©æ”¾æ–¹å¼è®¡ç®—ç›®æ ‡å°ºå¯¸
            orig_pixels = orig_height * orig_width
            
            if scale_definition == "width":
                target_width = definition_value
                if keep_proportions:
                    target_height = max(1, int(round(orig_height * (target_width / orig_width))))
                else:
                    target_height = orig_height
                    
            elif scale_definition == "height":
                target_height = definition_value
                if keep_proportions:
                    target_width = max(1, int(round(orig_width * (target_height / orig_height))))
                else:
                    target_width = orig_width
                    
            elif scale_definition == "longest_side":
                if orig_height >= orig_width:
                    target_height = definition_value
                    if keep_proportions:
                        target_width = max(1, int(round(orig_width * (target_height / orig_height))))
                    else:
                        target_width = orig_width
                else:
                    target_width = definition_value
                    if keep_proportions:
                        target_height = max(1, int(round(orig_height * (target_width / orig_width))))
                    else:
                        target_height = orig_height
                        
            elif scale_definition == "shortest_side":
                if orig_height <= orig_width:
                    target_height = definition_value
                    if keep_proportions:
                        target_width = max(1, int(round(orig_width * (target_height / orig_height))))
                    else:
                        target_width = orig_width
                else:
                    target_width = definition_value
                    if keep_proportions:
                        target_height = max(1, int(round(orig_height * (target_width / orig_width))))
                    else:
                        target_height = orig_height
                        
            elif scale_definition == "total_pixels":
                # ä¾æ®æ€»åƒç´ è®¡ç®—ç¼©æ”¾å› å­
                scale_factor = (definition_value / orig_pixels) ** 0.5
                target_width = max(1, int(round(orig_width * scale_factor)))
                target_height = max(1, int(round(orig_height * scale_factor)))
                
                # è‹¥keep_proportionsï¼Œç»†è°ƒä»¥æ›´æ¥è¿‘ç›®æ ‡åƒç´ 
                if keep_proportions:
                    actual_pixels = target_width * target_height
                    if abs(actual_pixels - definition_value) / definition_value > 0.1:
                        alternative_width = max(1, int(round((definition_value * orig_width / orig_height) ** 0.5)))
                        alternative_height = max(1, int(round(definition_value / alternative_width)))
                        if abs(alternative_width * alternative_height - definition_value) < abs(actual_pixels - definition_value):
                            target_width, target_height = alternative_width, alternative_height
        
        # ä¿è¯ç›®æ ‡å°ºå¯¸è‡³å°‘ä¸º 1x1
        target_width = max(1, target_width)
        target_height = max(1, target_height)
        
        # è®°å½•åŸå§‹è®¾å¤‡ä¸æ•°æ®ç±»å‹
        device = mask.device
        dtype = mask.dtype
        
        # å½“é€‰æ‹© Lanczos æ—¶ï¼Œä½¿ç”¨ PIL è·å¾—æ›´é«˜è´¨é‡
        if interpolation == "lanczos":
            scaled_masks = []
            for i in range(batch_size):
                # è½¬ä¸º PIL å›¾åƒ
                mask_np = mask[i].cpu().numpy() * 255
                mask_pil = Image.fromarray(mask_np.astype(np.uint8), mode='L')
                
                # Lanczos æ’å€¼ç¼©æ”¾
                mask_pil = mask_pil.resize((target_width, target_height), Image.LANCZOS)
                
                # è½¬å›å¼ é‡
                mask_np = np.array(mask_pil).astype(np.float32) / 255.0
                mask_tensor = torch.from_numpy(mask_np).to(device=device, dtype=dtype)
                scaled_masks.append(mask_tensor)
            
            scaled_mask = torch.stack(scaled_masks)
        else:
            # é€‰æ‹© PyTorch çš„æ’å€¼æ¨¡å¼
            if interpolation == "nearest":
                mode = "nearest"
            elif interpolation == "bilinear":
                mode = "bilinear"
            else:  # bicubic
                mode = "bicubic"
            
            # è½¬ä¸º float ä»¥ä¾¿æ’å€¼
            mask_float = mask.float()
            
            # è¿›è¡Œå°ºå¯¸å˜æ¢
            scaled_mask = torch.nn.functional.interpolate(
                mask_float.unsqueeze(1),
                size=(target_height, target_width),
                mode=mode,
                align_corners=False if mode != "nearest" else None
            ).squeeze(1)
            
            # è½¬å›åŸ dtype
            if dtype != torch.float32:
                scaled_mask = scaled_mask.to(dtype)
        
        # ä¿è¯æ•°å€¼åœ¨ 0-1 èŒƒå›´
        scaled_mask = torch.clamp(scaled_mask, 0.0, 1.0)
        
        return (scaled_mask, target_width, target_height)


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "ImageRotation": ImageRotation,
    "ImageFlipping": ImageFlipping,
    "ImageScaling": ImageScaling,
    "MaskScale": MaskScale
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "ImageRotation": "å›¾åƒæ—‹è½¬",
    "ImageFlipping": "å›¾åƒç¿»è½¬",
    "ImageScaling": "å›¾åƒç¼©æ”¾",
    "MaskScale": "é®ç½©ç¼©æ”¾"
}
