import numpy as np
import torch
import cv2
from collections import defaultdict
from scipy import ndimage
from skimage import measure

class MaskSplitter:
    """
    é®ç½©æ‹†åˆ†Qï¼šé«˜æ•ˆå¯é çš„é®ç½©æ‹†åˆ†å·¥å…·
    - æ”¯æŒæœ€å°ç»„ä»¶è¿‡æ»¤ã€å°åŒºåŸŸåˆå¹¶/ç§»é™¤/ä¿ç•™
    - æä¾›æ–‡å­—/ç»“æ„ä¿æŠ¤çš„æ™ºèƒ½åˆå¹¶
    - è¾“å‡ºç»„ä»¶é®ç½©åˆ—è¡¨ï¼ˆå¯ä¿ç•™åŸå§‹åƒç´ å€¼ï¼‰
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "mask": ("MASK",),
                "min_component_size": ("INT", {
                    "default": 100,
                    "min": 1,
                    "max": 10000,
                    "step": 10,
                    "display": "slider|min_component_size (åƒç´ æ•°)"
                }),
                "small_region_handling": (["merge", "remove", "keep"], {
                    "default": "merge",
                    "display": "small_region_handling"
                }),
                "merge_distance_ratio": ("FLOAT", {
                    "default": 2.0,
                    "min": 0.1,
                    "max": 10.0,
                    "step": 0.1,
                    "display": "slider|merge_distance_ratio"
                }),
                "text_preservation": (["auto", "aggressive", "chinese_optimized", "disabled"], {
                    "default": "chinese_optimized",
                    "display": "text_preservation"
                }),
                "structure_preservation": (["disabled", "auto", "enhanced"], {
                    "default": "auto",
                    "display": "structure_preservation"
                }),
                "output_all_components": ("BOOLEAN", {
                    "default": True,
                    "label_on": "å¯ç”¨",
                    "label_off": "ç¦ç”¨",
                    "display": "output_all_components"
                }),
                "preserve_original_values": ("BOOLEAN", {
                    "default": True,
                    "label_on": "å¯ç”¨",
                    "label_off": "ç¦ç”¨",
                    "display": "preserve_original_values"
                }),
            },
            "optional": {
                "reference_image": ("IMAGE", {
                    "display": "reference_image (å¯é€‰)"
                }),
            }
        }

    # ä¿ç•™ INPUT_NAMES ä½œä¸ºå¤‡ä»½
    INPUT_NAMES = {
        "mask": "è¾“å…¥é®ç½©",
        "min_component_size": "min_component_size (åƒç´ æ•°)",
        "small_region_handling": "small_region_handling",
        "merge_distance_ratio": "merge_distance_ratio",
        "text_preservation": "text_preservation",
        "structure_preservation": "structure_preservation",
        "output_all_components": "output_all_components",
        "preserve_original_values": "preserve_original_values",
        "reference_image": "reference_image (å¯é€‰)",
    }

    RETURN_TYPES = ("MASK",)
    RETURN_NAMES = ("mask_list",)
    FUNCTION = "split"
    OUTPUT_IS_LIST = (True,)
    CATEGORY = "ğŸ¨QING/é®ç½©å¤„ç†"

    def split(self, mask, min_component_size=100, small_region_handling="merge", 
              merge_distance_ratio=2.0, text_preservation="auto", 
              structure_preservation="auto", output_all_components=True,
              preserve_original_values=True, reference_image=None):
        
        # è½¬æ¢maskä¸ºnumpyæ•°ç»„
        mask_np = self.mask_to_numpy(mask)
        original_mask_np = mask_np.copy()  # ä¿å­˜åŸå§‹é®ç½©å€¼
        
        # äºŒå€¼åŒ–ç”¨äºåˆ†æï¼Œä½†ä¿ç•™åŸå§‹å€¼ç”¨äºè¾“å‡º
        _, binary_mask = cv2.threshold(mask_np, 0.5, 1, cv2.THRESH_BINARY)
        binary_mask = binary_mask.astype(np.uint8)
        
        # å½¢æ€å­¦é¢„å¤„ç†ï¼šé’ˆå¯¹æ–‡å­—ä¼˜åŒ–
        if text_preservation != "disabled":
            binary_mask = self.preprocess_for_text(binary_mask)
        
        # ä½¿ç”¨è¿é€šç»„ä»¶åˆ†æç¡®ä¿æ‰€æœ‰åŒºåŸŸéƒ½è¢«æ‰¾åˆ°
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(
            binary_mask, connectivity=8
        )
        
        # æ”¶é›†æ‰€æœ‰ç»„ä»¶
        components = []
        for i in range(1, num_labels):  # è·³è¿‡èƒŒæ™¯(0)
            area = stats[i, cv2.CC_STAT_AREA]
            component_mask = (labels == i).astype(np.uint8)
            
            # ä¿å­˜åŸå§‹åƒç´ å€¼
            if preserve_original_values:
                component_original_values = original_mask_np * component_mask
            else:
                component_original_values = component_mask.astype(np.float32)
                
            components.append((i, component_original_values, area, centroids[i]))
        
        # å¤„ç†å°åŒºåŸŸ
        if small_region_handling != "keep":
            components = self.handle_small_regions(components, min_component_size, 
                                                 small_region_handling, merge_distance_ratio,
                                                 binary_mask.shape)
        
        # å¦‚æœå¯ç”¨äº†æ–‡å­—ä¿æŠ¤ï¼Œå°è¯•åˆå¹¶å¯èƒ½æ˜¯æ–‡å­—çš„ç»„ä»¶
        if text_preservation != "disabled" and len(components) > 1:
            components = self.preserve_text_components(components, binary_mask, text_preservation)
        
        # å¦‚æœå¯ç”¨äº†ç»“æ„ä¿æŠ¤ï¼Œå°è¯•åˆå¹¶å¯èƒ½å±äºåŒä¸€ç»“æ„çš„ç»„ä»¶
        if structure_preservation != "disabled" and len(components) > 1:
            components = self.preserve_structure_components(components, binary_mask, structure_preservation)
        
        # ç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½è¢«è¾“å‡º
        if output_all_components:
            # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰ç»„ä»¶çš„ç»¼åˆé®ç½©ï¼Œç¡®ä¿æ²¡æœ‰é—æ¼
            combined_mask = np.zeros_like(binary_mask, dtype=np.float32)
            for comp in components:
                if len(comp) == 4:
                    _, comp_mask, _, _ = comp
                else:
                    _, comp_mask, _, _, _ = comp
                combined_mask = np.maximum(combined_mask, comp_mask)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„åƒç´ 
            missing_pixels = np.sum(binary_mask) - np.sum(combined_mask > 0)
            if missing_pixels > 0:
                # åˆ›å»ºä¸€ä¸ªé¢å¤–çš„ç»„ä»¶åŒ…å«æ‰€æœ‰é—æ¼çš„åƒç´ 
                missing_mask = (binary_mask - (combined_mask > 0).astype(np.uint8))
                missing_mask[missing_mask < 0] = 0
                
                if preserve_original_values:
                    missing_values = original_mask_np * missing_mask
                else:
                    missing_values = missing_mask.astype(np.float32)
                    
                components.append((num_labels, missing_values, np.sum(missing_mask), (0, 0)))
        
        # è½¬æ¢ä¸ºè¾“å‡ºæ ¼å¼
        output_masks = []
        for comp in components:
            if len(comp) == 4:
                _, comp_mask, _, _ = comp
            else:
                _, comp_mask, _, _, _ = comp
            output_masks.append(torch.from_numpy(comp_mask).unsqueeze(0))
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»„ä»¶ï¼Œè¿”å›åŸå§‹é®ç½©
        if len(output_masks) == 0:
            output_masks = [mask]
        
        return (output_masks,)
    
    def mask_to_numpy(self, mask):
        """å°†å„ç§æ ¼å¼çš„maskè½¬æ¢ä¸ºnumpyæ•°ç»„"""
        if len(mask.shape) == 4:  # (B, H, W, 1)
            return mask[0, :, :, 0].cpu().numpy()
        elif len(mask.shape) == 3:
            if mask.shape[0] == 1:  # (1, H, W)
                return mask[0].cpu().numpy()
            else:  # (H, W, 1) or (H, W)
                return mask[:, :, 0].cpu().numpy() if mask.shape[2] == 1 else mask.cpu().numpy()
        else:  # (H, W)
            return mask.cpu().numpy()
    
    def handle_small_regions(self, components, min_size, handling_method, distance_ratio, mask_shape):
        """å¤„ç†å°åŒºåŸŸï¼šåˆå¹¶ã€ç§»é™¤æˆ–ä¿ç•™"""
        if len(components) <= 1:
            return components
        
        # åˆ†ç¦»å¤§å°ç»„ä»¶
        large_components = []
        small_components = []
        
        for comp in components:
            if len(comp) == 4:
                comp_id, comp_mask, area, centroid = comp
            else:
                comp_id, comp_mask, area, centroid, _ = comp
                
            if area >= min_size:
                large_components.append((comp_id, comp_mask, area, centroid))
            else:
                small_components.append((comp_id, comp_mask, area, centroid))
        
        # å¦‚æœæ²¡æœ‰å°åŒºåŸŸï¼Œç›´æ¥è¿”å›
        if not small_components:
            return components
            
        # æ ¹æ®å¤„ç†æ–¹å¼å¤„ç†å°åŒºåŸŸ
        if handling_method == "remove":
            # ç›´æ¥ç§»é™¤å°åŒºåŸŸ
            return large_components
        elif handling_method == "merge" and large_components:
            # åˆå¹¶å°åŒºåŸŸåˆ°æœ€è¿‘çš„å¤§åŒºåŸŸ
            return self.merge_small_regions(large_components, small_components, distance_ratio, mask_shape)
        else:
            # ä¿ç•™æ‰€æœ‰åŒºåŸŸ
            return large_components + small_components
    
    def merge_small_regions(self, large_comps, small_comps, distance_ratio, mask_shape):
        """åˆå¹¶å°åŒºåŸŸåˆ°æœ€è¿‘çš„å¤§åŒºåŸŸ"""
        if not large_comps or not small_comps:
            return large_comps + small_comps
            
        # è®¡ç®—å¹³å‡ç»„ä»¶å¤§å°ï¼Œç”¨äºç¡®å®šåˆå¹¶è·ç¦»
        avg_height = mask_shape[0] * 0.05  # ä½¿ç”¨å›¾åƒé«˜åº¦çš„5%ä½œä¸ºåŸºå‡†
        merge_distance = avg_height * distance_ratio
        
        # ä¸ºæ¯ä¸ªå¤§ç»„ä»¶åˆ›å»ºKDæ ‘ï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨åˆ—è¡¨ï¼‰
        large_centroids = [centroid for _, _, _, centroid in large_comps]
        
        # å¤„ç†æ¯ä¸ªå°åŒºåŸŸ
        merged_components = large_comps.copy()
        
        for small_id, small_mask, small_area, small_centroid in small_comps:
            # æ‰¾åˆ°æœ€è¿‘çš„å¤§ç»„ä»¶
            min_distance = float('inf')
            nearest_index = -1
            
            for i, (_, _, _, large_centroid) in enumerate(merged_components):
                dx = small_centroid[0] - large_centroid[0]
                dy = small_centroid[1] - large_centroid[1]
                distance = np.sqrt(dx*dx + dy*dy)
                
                if distance < min_distance:
                    min_distance = distance
                    nearest_index = i
            
            # å¦‚æœè·ç¦»åœ¨é˜ˆå€¼å†…ï¼Œåˆå¹¶åˆ°æœ€è¿‘çš„å¤§ç»„ä»¶
            if min_distance <= merge_distance and nearest_index >= 0:
                # è·å–å¤§ç»„ä»¶
                large_id, large_mask, large_area, large_centroid = merged_components[nearest_index]
                
                # åˆå¹¶é®ç½©
                merged_mask = np.maximum(large_mask, small_mask)
                
                # æ›´æ–°ç»„ä»¶
                merged_components[nearest_index] = (large_id, merged_mask, large_area + small_area, large_centroid)
            else:
                # è·ç¦»å¤ªè¿œï¼Œä¿ç•™ä¸ºç‹¬ç«‹ç»„ä»¶
                merged_components.append((small_id, small_mask, small_area, small_centroid))
        
        return merged_components
    
    def preserve_text_components(self, components, original_mask, mode):
        """å°è¯•ä¿æŒæ–‡å­—ç»„ä»¶çš„å®Œæ•´æ€§"""
        if len(components) <= 1:
            return components
        
        # ç»Ÿä¸€ç»„ä»¶æ ¼å¼
        unified_components = []
        for idx, comp in enumerate(components):
            if len(comp) == 4:
                comp_id, comp_mask, area, centroid = comp
                unified_components.append((comp_id, comp_mask, area, centroid, idx))
            else:
                comp_id, comp_mask, area, centroid, _ = comp
                unified_components.append((comp_id, comp_mask, area, centroid, idx))
        
        # è®¡ç®—ç»„ä»¶ä¹‹é—´çš„ç©ºé—´å…³ç³»
        text_candidates = []
        non_text_components = []
        
        for i, (comp_id, comp_mask, area, centroid, idx) in enumerate(unified_components):
            # ä½¿ç”¨ç®€å•çš„å¯å‘å¼æ–¹æ³•è¯†åˆ«å¯èƒ½æ˜¯æ–‡å­—çš„ç»„ä»¶
            is_text = self.is_likely_text(comp_mask, area)
            if is_text:
                text_candidates.append((comp_id, comp_mask, area, centroid, idx))
            else:
                non_text_components.append((comp_id, comp_mask, area, centroid, idx))
        
        # å¦‚æœæ²¡æœ‰æ–‡å­—å€™é€‰ï¼Œç›´æ¥è¿”å›æ‰€æœ‰ç»„ä»¶
        if not text_candidates:
            return components
        
        # æ ¹æ®æ¨¡å¼åˆå¹¶æ–‡å­—ç»„ä»¶
        if mode == "aggressive":
            # å°†æ‰€æœ‰æ–‡å­—ç»„ä»¶åˆå¹¶ä¸ºä¸€ä¸ª
            merged_text_mask = np.zeros_like(original_mask, dtype=np.float32)
            for comp_id, comp_mask, _, _, _ in text_candidates:
                merged_text_mask = np.maximum(merged_text_mask, comp_mask)
            
            # åˆ›å»ºä¸€ä¸ªæ–°çš„åˆå¹¶ç»„ä»¶
            merged_area = np.sum(merged_text_mask > 0)
            merged_centroid = self.calculate_centroid((merged_text_mask > 0).astype(np.uint8))
            
            # åˆ›å»ºæ–°çš„ç»„ä»¶åˆ—è¡¨
            new_components = [(0, merged_text_mask, merged_area, merged_centroid)]
            for comp_id, comp_mask, area, centroid, idx in non_text_components:
                new_components.append((comp_id, comp_mask, area, centroid))
            
            return new_components
        elif mode == "chinese_optimized":
            # ä¸“é—¨é’ˆå¯¹ä¸­æ–‡å­—ç¬¦ä¼˜åŒ–çš„å¤„ç†æ¨¡å¼
            return self.chinese_optimized_grouping(text_candidates, non_text_components, original_mask)
        else:  # auto mode
            # å°è¯•å°†ç©ºé—´ä¸Šæ¥è¿‘çš„æ–‡å­—ç»„ä»¶åˆ†ç»„
            text_groups = self.group_text_components(text_candidates)
            
            # åˆå¹¶æ¯ç»„æ–‡å­—ç»„ä»¶
            merged_components = []
            for group in text_groups:
                if len(group) == 1:
                    comp_id, comp_mask, area, centroid, idx = group[0]
                    merged_components.append((comp_id, comp_mask, area, centroid))
                else:
                    # åˆå¹¶ç»„å†…çš„æ‰€æœ‰ç»„ä»¶
                    merged_mask = np.zeros_like(original_mask, dtype=np.float32)
                    for comp_id, comp_mask, _, _, _ in group:
                        merged_mask = np.maximum(merged_mask, comp_mask)
                    
                    merged_area = np.sum(merged_mask > 0)
                    merged_centroid = self.calculate_centroid((merged_mask > 0).astype(np.uint8))
                    merged_components.append((0, merged_mask, merged_area, merged_centroid))
            
            # æ·»åŠ éæ–‡å­—ç»„ä»¶
            for comp_id, comp_mask, area, centroid, idx in non_text_components:
                merged_components.append((comp_id, comp_mask, area, centroid))
            
            return merged_components
    
    def preserve_structure_components(self, components, original_mask, mode):
        """
        å°è¯•ä¿æŒå›¾å½¢ç»“æ„çš„å®Œæ•´æ€§
        æ–°å¢å‡½æ•°ï¼šç”¨äºå¤„ç†éæ–‡å­—ä½†å±äºåŒä¸€ç»“æ„çš„æƒ…å†µ
        """
        if len(components) <= 1:
            return components

        # ç»Ÿä¸€ç»„ä»¶æ ¼å¼
        unified_components = []
        for idx, comp in enumerate(components):
            if len(comp) == 4:
                comp_id, comp_mask, area, centroid = comp
                unified_components.append((comp_id, comp_mask, area, centroid, idx))
            else:
                comp_id, comp_mask, area, centroid, _ = comp
                unified_components.append((comp_id, comp_mask, area, centroid, idx))

        # è®¡ç®—æ¯ä¸ªç»„ä»¶çš„ç‰¹å¾ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦å¯èƒ½å±äºåŒä¸€ç»“æ„
        processed_components = []
        used_indices = set()

        # è®¡ç®—åŸå›¾çš„é«˜åº¦ï¼Œç”¨äºè·ç¦»é˜ˆå€¼åˆ¤æ–­
        image_height = original_mask.shape[0]
        distance_threshold = image_height * 0.05  # è·ç¦»é˜ˆå€¼è®¾ä¸ºå›¾åƒé«˜åº¦çš„5%

        for i, (comp_id, comp_mask, area, centroid, idx) in enumerate(unified_components):
            if i in used_indices:
                continue

            current_group = [(comp_id, comp_mask, area, centroid, idx)]
            used_indices.add(i)

            # è®¡ç®—å½“å‰ç»„ä»¶çš„ç‰¹å¾
            binary_comp_mask = (comp_mask > 0).astype(np.uint8)
            current_contour = self.get_largest_contour(binary_comp_mask)
            current_hu_moments = cv2.HuMoments(cv2.moments(current_contour)).flatten() if current_contour is not None else np.zeros(7)

            for j, (other_id, other_mask, other_area, other_centroid, other_idx) in enumerate(unified_components):
                if j in used_indices or i == j:
                    continue

                binary_other_mask = (other_mask > 0).astype(np.uint8)
                other_contour = self.get_largest_contour(binary_other_mask)
                if other_contour is None:
                    continue

                # è®¡ç®—ä¸¤ä¸ªç»„ä»¶ä¹‹é—´çš„è·ç¦»
                dx = centroid[0] - other_centroid[0]
                dy = centroid[1] - other_centroid[1]
                distance = np.sqrt(dx*dx + dy*dy)

                # å¦‚æœè·ç¦»å¤ªè¿œï¼Œåˆ™ä¸å¤ªå¯èƒ½å±äºåŒä¸€ç»“æ„
                if distance > distance_threshold:
                    continue

                # è®¡ç®—HuçŸ©ç›¸ä¼¼æ€§ï¼ˆå½¢çŠ¶ç›¸ä¼¼æ€§ï¼‰
                other_hu_moments = cv2.HuMoments(cv2.moments(other_contour)).flatten()
                shape_similarity = np.linalg.norm(current_hu_moments - other_hu_moments)

                # è®¡ç®—é¢ç§¯æ¯”ä¾‹
                area_ratio = max(area, other_area) / min(area, other_area) if min(area, other_area) > 0 else float('inf')

                # åˆ¤æ–­æ˜¯å¦å¯èƒ½å±äºåŒä¸€ç»“æ„çš„æ¡ä»¶
                if (shape_similarity < 0.5 and  # å½¢çŠ¶ç›¸ä¼¼
                    area_ratio < 10 and         # é¢ç§¯ç›¸å·®ä¸å¤ªå¤§
                    distance < distance_threshold):  # è·ç¦»æ¥è¿‘

                    current_group.append((other_id, other_mask, other_area, other_centroid, other_idx))
                    used_indices.add(j)

            # æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦åˆå¹¶ç»„å†…çš„ç»„ä»¶
            if mode == "enhanced" or (mode == "auto" and len(current_group) > 1):
                # åˆå¹¶ç»„å†…çš„æ‰€æœ‰ç»„ä»¶
                merged_mask = np.zeros_like(original_mask, dtype=np.float32)
                for comp_data in current_group:
                    _, comp_mask, _, _, _ = comp_data
                    merged_mask = np.maximum(merged_mask, comp_mask)
                
                merged_area = np.sum(merged_mask > 0)
                merged_centroid = self.calculate_centroid((merged_mask > 0).astype(np.uint8))
                processed_components.append((0, merged_mask, merged_area, merged_centroid))
            else:
                # ä¸åˆå¹¶ï¼Œä¿æŒåŸæ ·
                for comp_data in current_group:
                    comp_id, comp_mask, area, centroid, idx = comp_data
                    processed_components.append((comp_id, comp_mask, area, centroid))

        # æ·»åŠ æœªåˆ†ç»„çš„ç»„ä»¶
        for i, (comp_id, comp_mask, area, centroid, idx) in enumerate(unified_components):
            if i not in used_indices:
                processed_components.append((comp_id, comp_mask, area, centroid))

        return processed_components

    def is_likely_text(self, mask, area):
        """æ”¹è¿›çš„æ–‡å­—è¯†åˆ«ç®—æ³•ï¼Œç‰¹åˆ«ä¼˜åŒ–ä¸­æ–‡å­—ç¬¦è¯†åˆ«"""
        # åˆ›å»ºäºŒå€¼æ©ç ç”¨äºè½®å»“æ£€æµ‹
        binary_mask = (mask > 0).astype(np.uint8)
        
        # è®¡ç®—è½®å»“
        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not contours:
            return False
        
        # è·å–æœ€å¤§è½®å»“ï¼ˆä¸»è¦è½®å»“ï¼‰
        main_contour = max(contours, key=cv2.contourArea)
        
        # è·å–å¤–æ¥çŸ©å½¢å’Œè½®å»“ç‰¹å¾
        x, y, w, h = cv2.boundingRect(main_contour)
        contour_area = cv2.contourArea(main_contour)
        
        # è®¡ç®—å„ç§ç‰¹å¾
        aspect_ratio = w / max(h, 1)
        fill_ratio = area / (w * h) if w * h > 0 else 0
        contour_fill_ratio = contour_area / area if area > 0 else 0
        
        # è®¡ç®—è½®å»“çš„å¤æ‚åº¦ï¼ˆå‘¨é•¿ä¸é¢ç§¯çš„æ¯”å€¼ï¼‰
        perimeter = cv2.arcLength(main_contour, True)
        complexity = perimeter / max(np.sqrt(contour_area), 1) if contour_area > 0 else 0
        
        # è®¡ç®—è½®å»“çš„å‡¸æ€§ç¼ºé™·ï¼ˆæ–‡å­—é€šå¸¸æœ‰ä¸€äº›å‡¹é™·ï¼‰
        hull = cv2.convexHull(main_contour, returnPoints=False)
        defects = cv2.convexityDefects(main_contour, hull)
        defect_count = len(defects) if defects is not None else 0
        
        # è®¡ç®—è¾¹ç•Œæ¡†çš„ç´§å¯†åº¦
        rect_area = w * h
        bbox_fill_ratio = contour_area / rect_area if rect_area > 0 else 0
        
        # å¤šé‡åˆ¤æ–­æ¡ä»¶ï¼Œç‰¹åˆ«é€‚åˆä¸­æ–‡å­—ç¬¦
        # æ¡ä»¶1: åŸºæœ¬å½¢çŠ¶ç‰¹å¾
        basic_text_features = (
            0.3 < aspect_ratio < 3.0 and      # ä¸­æ–‡å­—ç¬¦é€šå¸¸æ¥è¿‘æ­£æ–¹å½¢
            0.3 < fill_ratio < 0.95 and       # å¡«å……ç‡ä¸èƒ½å¤ªä½æˆ–å¤ªé«˜
            0.4 < bbox_fill_ratio < 0.9        # è¾¹ç•Œæ¡†å¡«å……ç‡
        )
        
        # æ¡ä»¶2: å¤æ‚åº¦ç‰¹å¾ï¼ˆæ–‡å­—æœ‰ä¸€å®šå¤æ‚åº¦ä½†ä¸ä¼šè¿‡äºå¤æ‚ï¼‰
        complexity_features = (
            3 < complexity < 15 and           # é€‚ä¸­çš„å¤æ‚åº¦
            defect_count >= 2                 # æœ‰ä¸€å®šçš„å‡¹é™·ç‰¹å¾
        )
        
        # æ¡ä»¶3: å°ºå¯¸ç‰¹å¾ï¼ˆæ’é™¤è¿‡å°æˆ–è¿‡å¤§çš„ç»„ä»¶ï¼‰
        size_features = (
            area > 50 and                     # æœ€å°é¢ç§¯é˜ˆå€¼
            area < 50000 and                  # æœ€å¤§é¢ç§¯é˜ˆå€¼
            min(w, h) > 8                     # æœ€å°å°ºå¯¸
        )
        
        # æ¡ä»¶4: ç‰¹æ®Šçš„ä¸­æ–‡å­—ç¬¦ç‰¹å¾
        chinese_features = (
            0.6 < aspect_ratio < 1.4 and      # ä¸­æ–‡å­—ç¬¦æ¥è¿‘æ­£æ–¹å½¢
            0.4 < fill_ratio < 0.8 and        # ä¸­æ–‡å­—ç¬¦çš„å…¸å‹å¡«å……ç‡
            defect_count >= 1                 # ä¸­æ–‡å­—ç¬¦é€šå¸¸æœ‰ç¬”ç”»é—´éš™
        )
        
        # ç»¼åˆåˆ¤æ–­ï¼šæ»¡è¶³åŸºæœ¬ç‰¹å¾å’Œå°ºå¯¸ç‰¹å¾ï¼Œå¹¶ä¸”æ»¡è¶³å¤æ‚åº¦ç‰¹å¾æˆ–ä¸­æ–‡ç‰¹å¾
        is_text = (basic_text_features and size_features and 
                  (complexity_features or chinese_features))
        
        return is_text
    
    def group_text_components(self, text_components):
        """æ™ºèƒ½æ–‡å­—åˆ†ç»„ç®—æ³•ï¼Œç‰¹åˆ«ä¼˜åŒ–ä¸­æ–‡å­—ç¬¦å¤„ç†"""
        if len(text_components) <= 1:
            return [text_components]
        
        # è®¡ç®—ç»„ä»¶çš„è¯¦ç»†ç‰¹å¾
        component_features = []
        for comp_id, comp_mask, area, centroid, idx in text_components:
            binary_mask = (comp_mask > 0).astype(np.uint8)
            contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if contours:
                x, y, w, h = cv2.boundingRect(contours[0])
                component_features.append({
                    'id': comp_id,
                    'mask': comp_mask,
                    'area': area,
                    'centroid': centroid,
                    'idx': idx,
                    'bbox': (x, y, w, h),
                    'width': w,
                    'height': h,
                    'aspect_ratio': w / max(h, 1)
                })
        
        if not component_features:
            return [text_components]
        
        # è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯
        heights = [f['height'] for f in component_features]
        widths = [f['width'] for f in component_features]
        areas = [f['area'] for f in component_features]
        
        avg_height = np.mean(heights)
        median_height = np.median(heights)
        avg_width = np.mean(widths)
        median_area = np.median(areas)
        
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„åˆ†ç»„ç­–ç•¥
        groups = []
        used_indices = set()
        
        for i, feat1 in enumerate(component_features):
            if i in used_indices:
                continue
                
            group = [feat1]
            used_indices.add(i)
            
            for j, feat2 in enumerate(component_features):
                if j in used_indices or i == j:
                    continue
                
                # å¤šç»´åº¦ç›¸ä¼¼æ€§åˆ¤æ–­
                should_group = self.should_group_characters(feat1, feat2, avg_height, avg_width, median_area)
                
                if should_group:
                    group.append(feat2)
                    used_indices.add(j)
            
            # è½¬æ¢å›åŸå§‹æ ¼å¼
            group_converted = []
            for feat in group:
                group_converted.append((feat['id'], feat['mask'], feat['area'], feat['centroid'], feat['idx']))
            
            groups.append(group_converted)
        
        return groups
    
    def should_group_characters(self, feat1, feat2, avg_height, avg_width, median_area):
        """åˆ¤æ–­ä¸¤ä¸ªå­—ç¬¦ç»„ä»¶æ˜¯å¦åº”è¯¥åˆ†ç»„ï¼ˆåˆå¹¶ï¼‰"""
        # è®¡ç®—è·ç¦»
        dx = feat1['centroid'][0] - feat2['centroid'][0]
        dy = feat1['centroid'][1] - feat2['centroid'][1]
        distance = np.sqrt(dx*dx + dy*dy)
        
        # è®¡ç®—å°ºå¯¸ç›¸ä¼¼æ€§
        height_ratio = max(feat1['height'], feat2['height']) / max(min(feat1['height'], feat2['height']), 1)
        width_ratio = max(feat1['width'], feat2['width']) / max(min(feat1['width'], feat2['width']), 1)
        area_ratio = max(feat1['area'], feat2['area']) / max(min(feat1['area'], feat2['area']), 1)
        
        # è®¡ç®—ä½ç½®å…³ç³»
        horizontal_distance = abs(dx)
        vertical_distance = abs(dy)
        
        # åˆ¤æ–­æ˜¯å¦åœ¨åŒä¸€è¡Œï¼ˆä¸­æ–‡å­—ç¬¦é€šå¸¸åœ¨åŒä¸€æ°´å¹³çº¿ä¸Šï¼‰
        is_same_line = vertical_distance < avg_height * 0.3
        
        # åˆ¤æ–­æ°´å¹³é—´è·æ˜¯å¦åˆç†ï¼ˆä¸èƒ½å¤ªè¿œä¹Ÿä¸èƒ½å¤ªè¿‘ï¼‰
        reasonable_spacing = avg_width * 0.1 < horizontal_distance < avg_width * 2.0
        
        # ä¸­æ–‡å­—ç¬¦ç‰¹æœ‰çš„åˆ¤æ–­æ¡ä»¶
        chinese_grouping_conditions = (
            is_same_line and                    # åœ¨åŒä¸€æ°´å¹³çº¿ä¸Š
            reasonable_spacing and              # åˆç†çš„æ°´å¹³é—´è·
            height_ratio < 2.0 and             # é«˜åº¦ç›¸ä¼¼
            area_ratio < 3.0                   # é¢ç§¯ç›¸ä¼¼
        )
        
        # é€šç”¨çš„åˆ†ç»„æ¡ä»¶ï¼ˆé€‚ç”¨äºè¢«é”™è¯¯åˆ†å‰²çš„å­—ç¬¦ï¼‰
        broken_character_conditions = (
            distance < avg_height * 1.5 and    # è·ç¦»å¾ˆè¿‘
            height_ratio < 1.8 and             # é«˜åº¦æ¯”è¾ƒç›¸ä¼¼
            (vertical_distance < avg_height * 0.5 or horizontal_distance < avg_width * 0.8)  # å‚ç›´æˆ–æ°´å¹³è·ç¦»å¾ˆè¿‘
        )
        
        # é¿å…è¿‡åº¦åˆå¹¶ï¼šå¦‚æœä¸¤ä¸ªç»„ä»¶éƒ½æ¯”è¾ƒå¤§ä¸”è·ç¦»è¾ƒè¿œï¼Œä¸åˆå¹¶
        both_large = feat1['area'] > median_area and feat2['area'] > median_area
        far_apart = distance > avg_height * 1.2
        
        if both_large and far_apart:
            return False
        
        # æœ€ç»ˆåˆ¤æ–­ï¼šæ»¡è¶³ä¸­æ–‡åˆ†ç»„æ¡ä»¶æˆ–è€…æ˜¯è¢«åˆ†å‰²å­—ç¬¦çš„æ¡ä»¶
        return chinese_grouping_conditions or broken_character_conditions
    
    def chinese_optimized_grouping(self, text_candidates, non_text_components, original_mask):
        """ä¸“é—¨é’ˆå¯¹ä¸­æ–‡å­—ç¬¦ä¼˜åŒ–çš„åˆ†ç»„ç®—æ³•"""
        if not text_candidates:
            return [(comp_id, comp_mask, area, centroid) for comp_id, comp_mask, area, centroid, idx in non_text_components]
        
        # åˆ†ææ–‡å­—ç»„ä»¶çš„ç‰¹å¾ï¼Œè¯†åˆ«å¯èƒ½çš„ä¸­æ–‡å­—ç¬¦
        chinese_components = []
        other_text_components = []
        
        for comp_id, comp_mask, area, centroid, idx in text_candidates:
            if self.is_likely_chinese_character(comp_mask, area):
                chinese_components.append((comp_id, comp_mask, area, centroid, idx))
            else:
                other_text_components.append((comp_id, comp_mask, area, centroid, idx))
        
        # å¯¹ä¸­æ–‡ç»„ä»¶è¿›è¡Œç‰¹æ®Šå¤„ç†
        final_components = []
        
        if chinese_components:
            # ä½¿ç”¨ä¸“é—¨çš„ä¸­æ–‡å­—ç¬¦åˆ†ç»„é€»è¾‘
            chinese_groups = self.group_chinese_characters(chinese_components)
            
            for group in chinese_groups:
                if len(group) == 1:
                    # å•ä¸ªå­—ç¬¦ï¼Œç›´æ¥ä¿ç•™
                    comp_id, comp_mask, area, centroid, idx = group[0]
                    final_components.append((comp_id, comp_mask, area, centroid))
                else:
                    # å¤šä¸ªç»„ä»¶ï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶
                    should_merge = self.should_merge_chinese_group(group)
                    
                    if should_merge:
                        # åˆå¹¶ä¸ºä¸€ä¸ªå­—ç¬¦
                        merged_mask = np.zeros_like(original_mask, dtype=np.float32)
                        for comp_id, comp_mask, _, _, _ in group:
                            merged_mask = np.maximum(merged_mask, comp_mask)
                        
                        merged_area = np.sum(merged_mask > 0)
                        merged_centroid = self.calculate_centroid((merged_mask > 0).astype(np.uint8))
                        final_components.append((0, merged_mask, merged_area, merged_centroid))
                    else:
                        # ä¿æŒåˆ†ç¦»
                        for comp_id, comp_mask, area, centroid, idx in group:
                            final_components.append((comp_id, comp_mask, area, centroid))
            
            # åå¤„ç†ï¼šå¯¹å‰©ä½™çš„å°ç»„ä»¶è¿›è¡Œå¼ºåˆ¶åˆå¹¶
            final_components = self.post_process_small_fragments(final_components, original_mask)
        
        # å¤„ç†å…¶ä»–æ–‡å­—ç»„ä»¶
        for comp_id, comp_mask, area, centroid, idx in other_text_components:
            final_components.append((comp_id, comp_mask, area, centroid))
        
        # æ·»åŠ éæ–‡å­—ç»„ä»¶
        for comp_id, comp_mask, area, centroid, idx in non_text_components:
            final_components.append((comp_id, comp_mask, area, centroid))
        
        return final_components
    
    def is_likely_chinese_character(self, mask, area):
        """åˆ¤æ–­ç»„ä»¶æ˜¯å¦å¯èƒ½æ˜¯ä¸­æ–‡å­—ç¬¦æˆ–å…¶éƒ¨åˆ†ï¼ˆé™ä½é˜ˆå€¼ä»¥æ•è·åˆ†æ•£éƒ¨åˆ†ï¼‰"""
        binary_mask = (mask > 0).astype(np.uint8)
        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return False
        
        main_contour = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(main_contour)
        
        # ä¸­æ–‡å­—ç¬¦ç‰¹å¾ï¼ˆæ”¾å®½æ¡ä»¶ä»¥æ•è·åˆ†æ•£çš„éƒ¨åˆ†ï¼‰
        aspect_ratio = w / max(h, 1)
        
        # æ”¾å®½å®½é«˜æ¯”èŒƒå›´ï¼ŒåŒ…æ‹¬å¯èƒ½çš„å­—ç¬¦ç‰‡æ®µ
        is_reasonable_shape = 0.2 <= aspect_ratio <= 5.0  # æ›´å®½çš„èŒƒå›´
        
        # å¤§å¹…é™ä½é¢ç§¯é˜ˆå€¼ï¼ŒåŒ…æ‹¬å°çš„å­—ç¬¦ç‰‡æ®µ
        reasonable_size = 20 <= area <= 50000  # é™ä½æœ€å°é¢ç§¯ä»100åˆ°20
        
        # æ”¾å®½å¤æ‚åº¦è¦æ±‚
        perimeter = cv2.arcLength(main_contour, True)
        complexity = perimeter / max(np.sqrt(area), 1) if area > 0 else 0
        has_some_complexity = 2 <= complexity <= 30  # æ”¾å®½å¤æ‚åº¦èŒƒå›´
        
        # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæ˜¯å°é¢ç§¯ä½†å½¢çŠ¶åˆç†ï¼Œä¹Ÿè®¤ä¸ºæ˜¯å­—ç¬¦ç‰‡æ®µ
        is_small_fragment = (
            20 <= area <= 200 and 
            0.3 <= aspect_ratio <= 3.0 and
            complexity >= 1.5
        )
        
        # ä¸»è¦æ¡ä»¶æˆ–å°ç‰‡æ®µæ¡ä»¶
        main_condition = is_reasonable_shape and reasonable_size and has_some_complexity
        
        return main_condition or is_small_fragment
    
    def group_chinese_characters(self, chinese_components):
        """å¯¹ä¸­æ–‡å­—ç¬¦è¿›è¡Œåˆ†ç»„ï¼Œä½¿ç”¨æ›´ç§¯æçš„åˆå¹¶ç­–ç•¥"""
        if len(chinese_components) <= 1:
            return [chinese_components]
        
        # è®¡ç®—å­—ç¬¦é—´çš„å¹³å‡å°ºå¯¸å’Œç»Ÿè®¡ä¿¡æ¯
        heights = []
        widths = []
        areas = []
        bboxes = []
        
        for comp_id, comp_mask, area, centroid, idx in chinese_components:
            binary_mask = (comp_mask > 0).astype(np.uint8)
            contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                x, y, w, h = cv2.boundingRect(contours[0])
                heights.append(h)
                widths.append(w)
                areas.append(area)
                bboxes.append((x, y, w, h))
        
        if not heights:
            return [chinese_components]
        
        avg_height = np.mean(heights)
        avg_width = np.mean(widths)
        median_area = np.median(areas)
        max_dimension = max(avg_height, avg_width)
        
        
        # ä½¿ç”¨æ›´ç§¯æçš„åˆ†ç»„ç­–ç•¥
        groups = []
        used_indices = set()
        
        for i, (comp_id1, comp_mask1, area1, centroid1, idx1) in enumerate(chinese_components):
            if i in used_indices:
                continue
            
            group = [(comp_id1, comp_mask1, area1, centroid1, idx1)]
            used_indices.add(i)
            
            # é€’å½’æŸ¥æ‰¾å¯èƒ½å±äºåŒä¸€å­—ç¬¦çš„æ‰€æœ‰ç»„ä»¶
            self._find_related_components(i, chinese_components, group, used_indices, 
                                        avg_height, avg_width, median_area, max_dimension)
            
            groups.append(group)
        
        return groups
    
    def _find_related_components(self, base_idx, all_components, current_group, used_indices, 
                               avg_height, avg_width, median_area, max_dimension):
        """é€’å½’æŸ¥æ‰¾ä¸å½“å‰ç»„ç›¸å…³çš„ç»„ä»¶"""
        base_comp = all_components[base_idx]
        base_centroid = base_comp[3]
        base_area = base_comp[2]
        
        for j, (comp_id2, comp_mask2, area2, centroid2, idx2) in enumerate(all_components):
            if j in used_indices or j == base_idx:
                continue
            
            # è®¡ç®—è·ç¦»
            dx = base_centroid[0] - centroid2[0]
            dy = base_centroid[1] - centroid2[1]
            distance = np.sqrt(dx*dx + dy*dy)
            
            # å¤šé‡åˆ¤æ–­æ¡ä»¶ï¼ˆæ›´å®½æ¾çš„åˆå¹¶æ¡ä»¶ï¼‰
            should_merge = False
            
            # æ¡ä»¶1: éå¸¸æ¥è¿‘çš„ç»„ä»¶ï¼ˆå¯èƒ½æ˜¯åŒä¸€å­—ç¬¦çš„ä¸åŒéƒ¨åˆ†ï¼‰
            very_close = distance < max_dimension * 1.2  # å¢å¤§è·ç¦»é˜ˆå€¼
            
            # æ¡ä»¶2: å°ºå¯¸å…¼å®¹æ€§æ£€æŸ¥ï¼ˆæ”¾å®½è¦æ±‚ï¼‰
            area_ratio = max(base_area, area2) / max(min(base_area, area2), 1)
            size_compatible = area_ratio < 5.0  # æ”¾å®½é¢ç§¯æ¯”ä¾‹è¦æ±‚
            
            # æ¡ä»¶3: ä½ç½®åˆç†æ€§ï¼ˆåœ¨åˆç†çš„å­—ç¬¦èŒƒå›´å†…ï¼‰
            reasonable_position = (
                abs(dx) < max_dimension * 2.0 and  # æ°´å¹³è·ç¦»ä¸è¶…è¿‡2ä¸ªå­—ç¬¦å®½åº¦
                abs(dy) < max_dimension * 2.0      # å‚ç›´è·ç¦»ä¸è¶…è¿‡2ä¸ªå­—ç¬¦é«˜åº¦
            )
            
            # æ¡ä»¶4: ç‰¹æ®Šæƒ…å†µ - å°ç‰‡æ®µæ›´å®¹æ˜“åˆå¹¶
            is_small_fragment = min(base_area, area2) < median_area * 0.5
            if is_small_fragment:
                fragment_close = distance < max_dimension * 1.8  # å°ç‰‡æ®µå…è®¸æ›´å¤§è·ç¦»
                should_merge = fragment_close and reasonable_position
            else:
                should_merge = very_close and size_compatible and reasonable_position
            
            if should_merge:
                current_group.append((comp_id2, comp_mask2, area2, centroid2, idx2))
                used_indices.add(j)
                
                # é€’å½’æŸ¥æ‰¾ä¸æ–°åŠ å…¥ç»„ä»¶ç›¸å…³çš„å…¶ä»–ç»„ä»¶
                self._find_related_components(j, all_components, current_group, used_indices,
                                            avg_height, avg_width, median_area, max_dimension)
    
    def should_merge_chinese_group(self, group):
        """åˆ¤æ–­ä¸­æ–‡å­—ç¬¦ç»„æ˜¯å¦åº”è¯¥åˆå¹¶ï¼ˆæ›´ç§¯æçš„ç­–ç•¥ï¼‰"""
        if len(group) <= 1:
            return False
        
        # æ›´ç§¯æçš„åˆå¹¶ç­–ç•¥ï¼š
        # 1. å¦‚æœç»„å†…æœ‰å¤šä¸ªç»„ä»¶ï¼Œå¾ˆå¯èƒ½æ˜¯åŒä¸€å­—ç¬¦çš„åˆ†æ•£éƒ¨åˆ†
        # 2. åªè¦ä¸æ˜¯æ˜æ˜¾çš„å¤šä¸ªç‹¬ç«‹å­—ç¬¦å°±åˆå¹¶
        
        if len(group) <= 5:  # å…è®¸åˆå¹¶æœ€å¤š5ä¸ªç»„ä»¶
            # è®¡ç®—ç»„å†…ç»„ä»¶çš„ç©ºé—´åˆ†å¸ƒ
            centroids = [comp[3] for comp in group]  # æå–è´¨å¿ƒ
            areas = [comp[2] for comp in group]      # æå–é¢ç§¯
            
            # è®¡ç®—è¾¹ç•Œæ¡†
            min_x = min(c[0] for c in centroids)
            max_x = max(c[0] for c in centroids)
            min_y = min(c[1] for c in centroids)
            max_y = max(c[1] for c in centroids)
            
            bbox_width = max_x - min_x
            bbox_height = max_y - min_y
            
            # è®¡ç®—å¹³å‡ç»„ä»¶å°ºå¯¸
            avg_area = np.mean(areas)
            estimated_char_size = np.sqrt(avg_area)
            
            # åˆ¤æ–­æ˜¯å¦åœ¨åˆç†çš„å­—ç¬¦å°ºå¯¸èŒƒå›´å†…
            reasonable_bbox = (
                bbox_width <= estimated_char_size * 2.5 and  # å®½åº¦ä¸è¶…è¿‡2.5ä¸ªå­—ç¬¦
                bbox_height <= estimated_char_size * 2.5     # é«˜åº¦ä¸è¶…è¿‡2.5ä¸ªå­—ç¬¦
            )
            
            
            # å¦‚æœåœ¨åˆç†èŒƒå›´å†…ï¼Œåˆå¹¶
            if reasonable_bbox:
                return True
            
            # å³ä½¿è¾¹ç•Œæ¡†ç¨å¤§ï¼Œå¦‚æœç»„ä»¶æ•°é‡ä¸å¤šä¹Ÿåˆå¹¶ï¼ˆå¯èƒ½æ˜¯å¤æ‚å­—ç¬¦ï¼‰
            return len(group) <= 3
        
        # ç»„ä»¶å¤ªå¤šï¼Œå¯èƒ½æ˜¯å¤šä¸ªå­—ç¬¦ï¼Œä¸åˆå¹¶
        return False
    
    def post_process_small_fragments(self, components, original_mask):
        """åå¤„ç†ï¼šå¼ºåˆ¶åˆå¹¶å‰©ä½™çš„å°ç‰‡æ®µåˆ°æœ€è¿‘çš„å¤§ç»„ä»¶"""
        if len(components) <= 1:
            return components
        
        # è®¡ç®—ç»„ä»¶çš„é¢ç§¯ç»Ÿè®¡
        areas = [comp[2] for comp in components]
        median_area = np.median(areas)
        
        # åˆ†ç¦»å¤§ç»„ä»¶å’Œå°ç‰‡æ®µ
        large_components = []
        small_fragments = []
        
        for comp in components:
            comp_id, comp_mask, area, centroid = comp
            if area >= median_area * 0.3:  # é¢ç§¯å¤§äºä¸­ä½æ•°30%çš„è®¤ä¸ºæ˜¯ä¸»è¦ç»„ä»¶
                large_components.append(comp)
            else:
                small_fragments.append(comp)
        
        if not small_fragments or not large_components:
            return components
        
        
        # å°†å°ç‰‡æ®µåˆå¹¶åˆ°æœ€è¿‘çš„å¤§ç»„ä»¶
        final_components = []
        
        for large_comp in large_components:
            large_id, large_mask, large_area, large_centroid = large_comp
            merged_mask = large_mask.copy()
            
            # æŸ¥æ‰¾éœ€è¦åˆå¹¶åˆ°è¿™ä¸ªå¤§ç»„ä»¶çš„å°ç‰‡æ®µ
            fragments_to_merge = []
            for small_comp in small_fragments[:]:  # ä½¿ç”¨åˆ‡ç‰‡å¤åˆ¶ï¼Œå…è®¸ä¿®æ”¹åŸåˆ—è¡¨
                small_id, small_mask, small_area, small_centroid = small_comp
                
                # è®¡ç®—è·ç¦»
                dx = large_centroid[0] - small_centroid[0]
                dy = large_centroid[1] - small_centroid[1]
                distance = np.sqrt(dx*dx + dy*dy)
                
                # ä¼°è®¡å­—ç¬¦å°ºå¯¸
                estimated_size = np.sqrt(large_area)
                
                # å¦‚æœå°ç‰‡æ®µè·ç¦»å¤§ç»„ä»¶å¾ˆè¿‘ï¼Œåˆå¹¶å®ƒ
                if distance < estimated_size * 1.5:  # è·ç¦»é˜ˆå€¼
                    fragments_to_merge.append(small_comp)
                    small_fragments.remove(small_comp)  # ä»å¾…å¤„ç†åˆ—è¡¨ä¸­ç§»é™¤
            
            # åˆå¹¶æ‰¾åˆ°çš„ç‰‡æ®µ
            for fragment in fragments_to_merge:
                _, frag_mask, _, _ = fragment
                merged_mask = np.maximum(merged_mask, frag_mask)
            
            # æ›´æ–°ç»„ä»¶ä¿¡æ¯
            merged_area = np.sum(merged_mask > 0)
            merged_centroid = self.calculate_centroid((merged_mask > 0).astype(np.uint8))
            final_components.append((large_id, merged_mask, merged_area, merged_centroid))
        
        # æ·»åŠ æœªè¢«åˆå¹¶çš„å°ç‰‡æ®µï¼ˆè·ç¦»æ‰€æœ‰å¤§ç»„ä»¶éƒ½å¤ªè¿œï¼‰
        for remaining_fragment in small_fragments:
            final_components.append(remaining_fragment)
        
        return final_components

    def get_largest_contour(self, mask):
        """è·å–é®ç½©ä¸­æœ€å¤§çš„è½®å»“"""
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not contours:
            return None
        return max(contours, key=cv2.contourArea)
    
    def preprocess_for_text(self, binary_mask):
        """é’ˆå¯¹æ–‡å­—çš„å½¢æ€å­¦é¢„å¤„ç†ï¼Œæ›´ç§¯æåœ°è¿æ¥åˆ†æ•£çš„å­—ç¬¦éƒ¨åˆ†"""
        # å¤šçº§å½¢æ€å­¦å¤„ç†ç­–ç•¥
        
        # ç¬¬ä¸€çº§ï¼šä½¿ç”¨å°æ ¸è¿›è¡ŒåŸºç¡€è¿æ¥
        kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        processed = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel_small, iterations=1)
        
        # ç¬¬äºŒçº§ï¼šä½¿ç”¨ä¸­ç­‰å¤§å°çš„æ ¸è¿›è¡Œæ›´å¼ºçš„è¿æ¥
        kernel_medium = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        processed = cv2.morphologyEx(processed, cv2.MORPH_CLOSE, kernel_medium, iterations=1)
        
        # ç¬¬ä¸‰çº§ï¼šé’ˆå¯¹æ–‡å­—ç‰¹ç‚¹ï¼Œä½¿ç”¨æ°´å¹³å’Œå‚ç›´æ ¸åˆ†åˆ«å¤„ç†
        kernel_horizontal = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 3))
        kernel_vertical = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 7))
        
        # æ°´å¹³è¿æ¥ï¼ˆè¿æ¥å·¦å³åˆ†ç¦»çš„éƒ¨åˆ†ï¼‰
        h_processed = cv2.morphologyEx(processed, cv2.MORPH_CLOSE, kernel_horizontal, iterations=1)
        # å‚ç›´è¿æ¥ï¼ˆè¿æ¥ä¸Šä¸‹åˆ†ç¦»çš„éƒ¨åˆ†ï¼‰
        v_processed = cv2.morphologyEx(processed, cv2.MORPH_CLOSE, kernel_vertical, iterations=1)
        
        # åˆå¹¶ä¸¤ä¸ªæ–¹å‘çš„å¤„ç†ç»“æœ
        processed = cv2.bitwise_or(h_processed, v_processed)
        
        # æ¸…ç†å°å™ªå£°
        kernel_clean = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
        processed = cv2.morphologyEx(processed, cv2.MORPH_OPEN, kernel_clean, iterations=1)
        
        # æ£€æŸ¥å¤„ç†æ•ˆæœ
        original_components = self.count_components(binary_mask)
        processed_components = self.count_components(processed)
        
        
        # å¦‚æœç»„ä»¶æ•°é‡å‡å°‘å¤ªå¤šï¼ˆè¶…è¿‡80%ï¼‰ï¼Œå¯èƒ½è¿‡åº¦è¿æ¥äº†ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„ç­–ç•¥
        if processed_components < original_components * 0.2:
            # å›é€€åˆ°æ›´æ¸©å’Œçš„å¤„ç†
            kernel_gentle = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4, 4))
            processed = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel_gentle, iterations=2)
            processed = cv2.morphologyEx(processed, cv2.MORPH_OPEN, kernel_clean, iterations=1)
        
        return processed
    
    def count_components(self, binary_mask):
        """è®¡ç®—äºŒå€¼å›¾åƒä¸­çš„è¿é€šç»„ä»¶æ•°é‡"""
        num_labels, _, _, _ = cv2.connectedComponentsWithStats(binary_mask, connectivity=8)
        return num_labels - 1  # å‡å»èƒŒæ™¯ç»„ä»¶
    
    def calculate_centroid(self, mask):
        """è®¡ç®—é®ç½©çš„è´¨å¿ƒ"""
        # ç¡®ä¿maskæ˜¯æ•°å€¼ç±»å‹ï¼Œè€Œä¸æ˜¯å¸ƒå°”ç±»å‹
        if mask.dtype == bool:
            mask = mask.astype(np.uint8)
        
        moments = cv2.moments(mask)
        if moments["m00"] == 0:
            return (0, 0)
        cx = int(moments["m10"] / moments["m00"])
        cy = int(moments["m01"] / moments["m00"])
        return (cx, cy)

# è®©ComfyUIè¯†åˆ«è¿™ä¸ªèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "MaskSplitter": MaskSplitter
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "MaskSplitter": "é®ç½©æ‹†åˆ†"
}
