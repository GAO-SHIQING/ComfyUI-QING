# -*- coding: utf-8 -*-
"""
DeepSeek Language APIèŠ‚ç‚¹
è°ƒç”¨DeepSeekè¯­è¨€æ¨¡å‹APIè¿›è¡Œæ–‡æœ¬æ¨ç†
æ”¯æŒå¤šå¹³å°ï¼šç«å±±å¼•æ“ã€é˜¿é‡Œäº‘ç™¾ç‚¼ã€ç¡…åŸºæµåŠ¨
"""

import os
from typing import Optional, List, Dict, Any
from pathlib import Path

# å®šä¹‰æœåŠ¡å¹³å°é…ç½®
SERVICE_PLATFORMS = {
    "ç«å±±å¼•æ“": {
        "base_url": "https://ark.cn-beijing.volces.com/api/v3",
        "api_key_env": "VOLCENGINE_API_KEY",
        "config_key": "volcengine_api_key",
        "model_mapping": {
            "DeepSeek-V3.1": "deepseek-v3-1-250821",
            "DeepSeek-R1": "deepseek-r1-250528",
            "DeepSeek-V3": "deepseek-v3-250324"
        }
    },
    "é˜¿é‡Œäº‘ç™¾ç‚¼": {
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "api_key_env": "DASHSCOPE_API_KEY", 
        "config_key": "dashscope_api_key",
        "model_mapping": {
            "DeepSeek-V3.1": "deepseek-v3.1",        
            "DeepSeek-R1": "deepseek-r1-0528",       
            "DeepSeek-V3": "deepseek-v3"             
        }
    },
    "ç¡…åŸºæµåŠ¨": {
        "base_url": "https://api.siliconflow.cn/v1",
        "api_key_env": "SILICONFLOW_API_KEY",
        "config_key": "siliconflow_api_key",
        "model_mapping": {
            "DeepSeek-V3.1": "deepseek-ai/DeepSeek-V3.1",
            "DeepSeek-R1": "deepseek-ai/DeepSeek-R1",
            "DeepSeek-V3": "deepseek-ai/DeepSeek-V3"
        }
    }
}

# æ”¯æŒçš„DeepSeekæ¨¡å‹ï¼ˆæŒ‰ç…§éœ€æ±‚é¡ºåºï¼‰
DEEPSEEK_MODELS = [
    "DeepSeek-V3.1",
    "DeepSeek-R1", 
    "DeepSeek-V3"
]


class DeepSeekLanguageAPI:
    """
    DeepSeekè¯­è¨€æ¨¡å‹APIè°ƒç”¨èŠ‚ç‚¹
    æ”¯æŒå¤šä¸ªæœåŠ¡å¹³å°çš„DeepSeekæ¨¡å‹è°ƒç”¨
    """
    
    # ç±»å±æ€§
    SERVICE_PLATFORMS = SERVICE_PLATFORMS
    DEEPSEEK_MODELS = DEEPSEEK_MODELS
    
    def __init__(self):
        """åˆå§‹åŒ–èŠ‚ç‚¹"""
        self.conversation_history = {}
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text_input": ("STRING", {
                    "default": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
                    "multiline": True,
                    "tooltip": "è¾“å…¥è¦å‘é€ç»™DeepSeekæ¨¡å‹çš„æ–‡æœ¬å†…å®¹"
                }),
                "platform": (list(cls.SERVICE_PLATFORMS.keys()), {
                    "default": "ç¡…åŸºæµåŠ¨",
                    "tooltip": "é€‰æ‹©DeepSeek APIæœåŠ¡æä¾›å•†"
                }),
                "model": (cls.DEEPSEEK_MODELS, {
                    "default": "DeepSeek-V3.1",
                    "tooltip": "é€‰æ‹©è¦ä½¿ç”¨çš„DeepSeekæ¨¡å‹"
                }),
                       "max_tokens": ("INT", {
                           "default": 3072,
                           "min": 1,
                           "max": 32768,
                           "step": 1,
                           "tooltip": "æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶æœ€å¤šèƒ½ä½¿ç”¨çš„tokenæ•°é‡"
                       }),
                "history": ("INT", {
                    "default": 6,
                    "min": 1,
                    "max": 18,
                    "step": 1,
                    "tooltip": "ä¿æŒçš„å†å²å¯¹è¯è½®æ•°"
                }),
            },
            "optional": {
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œè¶Šé«˜è¶Šéšæœº"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.1,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§"
                }),
                "clear_history": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦æ¸…é™¤å†å²å¯¹è¯è®°å½•"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "INT")
    RETURN_NAMES = ("generated_text", "conversation_info", "token_count")
    FUNCTION = "generate_text"
    CATEGORY = "ğŸ¨QING/APIè°ƒç”¨"
    OUTPUT_NODE = False
    
    def _get_api_key(self, platform_config: Dict[str, str]) -> str:
        """è·å–APIå¯†é’¥ï¼ˆæ”¯æŒå¤šå¹³å°ï¼‰"""
        final_api_key = ""
        config_key = platform_config["config_key"]

        # ç­–ç•¥1: ä»ç¯å¢ƒå˜é‡è·å–
        final_api_key = os.getenv(platform_config["api_key_env"], '')

        # ç­–ç•¥2: ä»ğŸ¨QINGé…ç½®ç³»ç»Ÿè·å–ï¼ˆæ”¯æŒå¤šå¹³å°ï¼‰
        if not final_api_key:
            try:
                from .config_server import get_api_key_by_platform
                final_api_key = get_api_key_by_platform(config_key)
            except ImportError:
                try:
                    # å°è¯•ä»æ–°çš„é…ç½®ç®¡ç†å™¨è·å–
                    from .settings_approach import get_qing_api_key_improved
                    final_api_key = get_qing_api_key_improved(config_key)
                except ImportError:
                    pass

        # ç­–ç•¥3: å›é€€åˆ°æ—§ç‰ˆAPIå¯†é’¥æœåŠ¡ï¼ˆä»…æ”¯æŒGLMï¼‰
        if not final_api_key and config_key in ["glm_api_key", "zhipuai_api_key"]:
            try:
                from .api_key_server import get_qing_api_key
                final_api_key = get_qing_api_key()
            except ImportError:
                pass

        # ç­–ç•¥4: ä»ä¸´æ—¶æ–‡ä»¶è·å–ï¼ˆå…¼å®¹æ€§ï¼‰
        if not final_api_key:
            try:
                import tempfile
                temp_file = Path(tempfile.gettempdir()) / f"qing_{config_key}_temp.txt"
                if temp_file.exists():
                    with open(temp_file, 'r', encoding='utf-8') as f:
                        temp_key = f.read().strip()
                        if temp_key:
                            final_api_key = temp_key
            except Exception:
                pass

        return final_api_key
    
    def generate_text(self, text_input: str, platform: str, model: str, max_tokens: int, history: int,
                     temperature: Optional[float] = 0.7, top_p: Optional[float] = 0.9, 
                     clear_history: Optional[bool] = False):
        """
        è°ƒç”¨DeepSeek APIç”Ÿæˆæ–‡æœ¬
        
        Args:
            text_input: è¾“å…¥æ–‡æœ¬
            platform: æœåŠ¡å¹³å°
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§tokenæ•°
            history: ä¿æŒçš„å†å²è½®æ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: top_på‚æ•°
            clear_history: æ˜¯å¦æ¸…é™¤å†å²
            
        Returns:
            tuple: (ç”Ÿæˆçš„æ–‡æœ¬,)
        """
        try:
            # æ¸…é™¤å†å²è®°å½•
            if clear_history:
                self.conversation_history.clear()
            
            # è·å–å¹³å°é…ç½®
            platform_config = self.SERVICE_PLATFORMS.get(platform)
            if not platform_config:
                return (f"é”™è¯¯ï¼šä¸æ”¯æŒçš„å¹³å° '{platform}'", "é”™è¯¯çŠ¶æ€", 0)
            
            # è·å–APIå¯†é’¥
            api_key = self._get_api_key(platform_config)
            if not api_key:
                error_msg = f"""é”™è¯¯ï¼šæœªæä¾›{platform}çš„APIå¯†é’¥ã€‚è¯·å°è¯•ä»¥ä¸‹æ–¹æ³•ä¹‹ä¸€ï¼š

1. è®¾ç½®ç¯å¢ƒå˜é‡ï¼š{platform_config['api_key_env']}
2. åœ¨ğŸ¨QINGè®¾ç½®ä¸­é…ç½®APIå¯†é’¥
3. ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è®¾ç½®å¯†é’¥

è¯·é…ç½®åé‡è¯•ã€‚"""
                return (error_msg, "APIå¯†é’¥ç¼ºå¤±", 0)
            
            # å¯¼å…¥openaiåº“
            try:
                from openai import OpenAI
            except ImportError:
                return ("é”™è¯¯ï¼šæœªå®‰è£…openaiåº“ã€‚è¯·è¿è¡Œï¼špip install openai", "ä¾èµ–ç¼ºå¤±", 0)
            
            # åˆå§‹åŒ–å®¢æˆ·ç«¯
            client = OpenAI(
                base_url=platform_config["base_url"],
                api_key=api_key
            )
            
            # è·å–å¯¹è¯å†å²
            conversation_key = f"{platform}_{model}"
            if conversation_key not in self.conversation_history:
                self.conversation_history[conversation_key] = []
            
            current_history = self.conversation_history[conversation_key]
            
            # æ·»åŠ å½“å‰è¾“å…¥åˆ°å†å²
            current_history.append({"role": "user", "content": text_input})
            
            # é™åˆ¶å†å²é•¿åº¦
            if len(current_history) > history * 2:
                current_history = current_history[-(history * 2):]
                self.conversation_history[conversation_key] = current_history
            
            # æ ¹æ®å¹³å°è·å–å¯¹åº”çš„æ¨¡å‹åç§°
            platform_model_mapping = platform_config.get("model_mapping", {})
            api_model_name = platform_model_mapping.get(model, model.lower())
            
            # è°ƒç”¨API
            response = client.chat.completions.create(
                model=api_model_name,
                messages=current_history,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p
            )
            
            # æå–å›å¤
            reply = response.choices[0].message.content
            
            # æ·»åŠ å›å¤åˆ°å†å²
            current_history.append({"role": "assistant", "content": reply})
            
            # è®¡ç®—tokenä½¿ç”¨æƒ…å†µ
            token_count = response.usage.total_tokens if hasattr(response, 'usage') and response.usage else 0
            
            # ç”Ÿæˆå¯¹è¯ä¿¡æ¯
            conversation_info = f"å¹³å°: {platform} | æ¨¡å‹: {model} | å†å²è½®æ•°: {len(current_history)//2} | Tokens: {token_count}"
            
            return (reply, conversation_info, token_count)
            
        except Exception as e:
            error_message = f"DeepSeek APIè°ƒç”¨å¤±è´¥ ({platform}): {str(e)}"
            return (error_message, "é”™è¯¯çŠ¶æ€", 0)

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        # æ¯æ¬¡éƒ½é‡æ–°æ‰§è¡Œï¼Œå› ä¸ºæ˜¯APIè°ƒç”¨
        return float("nan")


# ComfyUIèŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "DeepSeekLanguageAPI": DeepSeekLanguageAPI
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "DeepSeekLanguageAPI": "DeepSeek_è¯­è¨€ä¸¨API"
}