# -*- coding: utf-8 -*-
"""
GLM Vision APIèŠ‚ç‚¹
è°ƒç”¨æ™ºè°±GLMè§†è§‰æ¨¡å‹APIè¿›è¡Œå›¾åƒç†è§£å’Œåˆ†æ
"""

import os
import base64
import io
from typing import Optional, List, Dict, Any
import torch
import numpy as np
from PIL import Image

try:
    from zai import ZhipuAiClient
    ZAI_SDK_AVAILABLE = True
except ImportError:
    ZAI_SDK_AVAILABLE = False


class GLMVisionAPI:
    """
    GLM Vision APIèŠ‚ç‚¹
    
    åŠŸèƒ½ï¼š
    - è°ƒç”¨æ™ºè°±GLMè§†è§‰æ¨¡å‹APIè¿›è¡Œå›¾åƒç†è§£
    - æ”¯æŒå¤šç§GLMè§†è§‰æ¨¡å‹é€‰æ‹©
    - æ”¯æŒå†å²å¯¹è¯ä¸Šä¸‹æ–‡
    - æ”¯æŒå›¾åƒ+æ–‡æœ¬å¤šæ¨¡æ€è¾“å…¥
    """
    
    # GLMè§†è§‰æ¨¡å‹åˆ—è¡¨ï¼ˆä¸¥æ ¼æŒ‰ç…§å®˜æ–¹æ–‡æ¡£å®šä¹‰ï¼‰
    GLM_VISION_MODELS = [
        # GLM-4.5V ç³»åˆ—ï¼ˆæœ€æ–°è§†è§‰æ¨¡å‹ï¼‰
        "glm-4.5v",
        
        # GLM-4.1V Thinking ç³»åˆ—ï¼ˆæ”¯æŒæ€ç»´é“¾æ¨ç†ï¼‰
        "glm-4.1v-thinking-flashx", 
        
        # GLM-4V ç³»åˆ—ï¼ˆç»å…¸è§†è§‰æ¨¡å‹ï¼‰
        "glm-4v-flash",  # å¿«é€Ÿç‰ˆæœ¬
        "glm-4v",        # æ ‡å‡†ç‰ˆæœ¬  
        "glm-4v-plus",   # å¢å¼ºç‰ˆæœ¬
    ]
    
    def __init__(self):
        self.conversation_history: List[Dict[str, Any]] = []
        self.client: Optional[Any] = None
        self._last_api_key: str = ""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE", {
                    "tooltip": "è¾“å…¥è¦åˆ†æçš„å›¾åƒ"
                }),
                "text_input": ("STRING", {
                    "default": "è¯·æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚",
                    "multiline": True,
                    "tooltip": "è¾“å…¥å…³äºå›¾åƒçš„é—®é¢˜æˆ–æŒ‡ä»¤"
                }),
                 "model": (cls.GLM_VISION_MODELS, {
                     "default": "glm-4.5v",
                     "tooltip": "é€‰æ‹©è¦ä½¿ç”¨çš„GLMè§†è§‰æ¨¡å‹"
                 }),
                "max_tokens": ("INT", {
                    "default": 3072,
                    "min": 1,
                    "max": 32768,
                    "step": 1,
                    "tooltip": "æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶æœ€å¤šèƒ½ä½¿ç”¨çš„tokenæ•°é‡"
                }),
                "history": ("INT", {
                    "default": 6,
                    "min": 1,
                    "max": 18,
                    "step": 1,
                    "tooltip": "ä¿æŒçš„å†å²å¯¹è¯è½®æ•°"
                }),
            },
            "optional": {
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œè¶Šé«˜è¶Šéšæœº"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.1,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§"
                }),
                "clear_history": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦æ¸…é™¤å†å²å¯¹è¯è®°å½•"
                }),
                "image_quality": (["auto", "low", "high"], {
                    "default": "auto",
                    "tooltip": "å›¾åƒè´¨é‡è®¾ç½®ï¼šautoè‡ªåŠ¨ï¼Œlowä½è´¨é‡ï¼Œhighé«˜è´¨é‡"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "INT")
    RETURN_NAMES = ("generated_text", "conversation_info", "token_count")
    FUNCTION = "analyze_image"
    CATEGORY = "ğŸ¨QING/APIè°ƒç”¨"
    OUTPUT_NODE = False
    
    def analyze_image(self, image: torch.Tensor, text_input: str, model: str, max_tokens: int, 
                     history: int, temperature: float = 0.7, 
                     top_p: float = 0.9, clear_history: bool = False, 
                     image_quality: str = "auto") -> tuple:
        """
        ä½¿ç”¨GLMè§†è§‰æ¨¡å‹åˆ†æå›¾åƒ
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒå¼ é‡
            text_input: è¾“å…¥æ–‡æœ¬
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§tokenæ•°
            history: å†å²å¯¹è¯è½®æ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: top_på‚æ•°
            clear_history: æ˜¯å¦æ¸…é™¤å†å²
            image_quality: å›¾åƒè´¨é‡
            
        è¿”å›:
            tuple: (ç”Ÿæˆçš„æ–‡æœ¬, å¯¹è¯ä¿¡æ¯, tokenä½¿ç”¨é‡)
        """
        try:
            # æ£€æŸ¥ä¾èµ–
            if not ZAI_SDK_AVAILABLE:
                error_msg = "é”™è¯¯ï¼šæœªå®‰è£…zai-sdkåº“ã€‚è¯·è¿è¡Œï¼špip install zai-sdk"
                return (error_msg, "ä¾èµ–ç¼ºå¤±", 0)
            
            # å¦‚æœéœ€è¦æ¸…é™¤å†å²
            if clear_history:
                self.conversation_history.clear()
            
            # è·å–APIå¯†é’¥ï¼ˆä¼˜å…ˆçº§ï¼šğŸ¨QINGè®¾ç½® > ç¯å¢ƒå˜é‡ï¼‰
            try:
                from .api_key_server import get_qing_api_key
                final_api_key = get_qing_api_key()
            except ImportError:
                # å›é€€åˆ°ç¯å¢ƒå˜é‡
                final_api_key = os.getenv('ZHIPUAI_API_KEY', '')
            
            if not final_api_key:
                error_msg = "é”™è¯¯ï¼šæœªæä¾›APIå¯†é’¥ã€‚è¯·åœ¨ğŸ¨QINGè®¾ç½®ä¸­é…ç½®æ™ºè°±GLM_API_Keyï¼Œæˆ–è®¾ç½®ZHIPUAI_API_KEYç¯å¢ƒå˜é‡"
                return (error_msg, "APIå¯†é’¥ç¼ºå¤±", 0)
            
            # åˆå§‹åŒ–å®¢æˆ·ç«¯
            if self.client is None or self._last_api_key != final_api_key:
                self.client = ZhipuAiClient(api_key=final_api_key)
                self._last_api_key = final_api_key
            
            # å¤„ç†å›¾åƒ
            image_base64 = self._process_image(image, image_quality)
            if image_base64 is None:
                error_msg = "é”™è¯¯ï¼šå›¾åƒå¤„ç†å¤±è´¥"
                return (error_msg, "å›¾åƒå¤„ç†å¼‚å¸¸", 0)
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = self._build_messages(text_input, image_base64, history)
            
            # æ ¹æ®å®˜æ–¹æ–‡æ¡£å¯¹ä¸åŒæ¨¡å‹è¿›è¡Œå‚æ•°é€‚é…
            response = None
            
            if model in ["glm-4.5v", "glm-4.1v-thinking-flashx"]:
                # GLM-4.5Vå’ŒGLM-4.1Vç³»åˆ—ï¼šæ”¯æŒå®Œæ•´å‚æ•°
                response = self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    stream=False
                )
            elif model in ["glm-4v-flash", "glm-4v", "glm-4v-plus"]:
                # GLM-4Vç³»åˆ—ï¼šåŸºäº1210é”™è¯¯æµ‹è¯•ç»“æœè¿›è¡Œå¤šå±‚å‚æ•°å°è¯•
                try:
                    # ç­–ç•¥1ï¼šåªä½¿ç”¨max_tokensï¼ˆå»æ‰temperatureå’Œtop_pï¼‰
                    response = self.client.chat.completions.create(
                        model=model,
                        messages=messages,
                        max_tokens=max_tokens,
                        stream=False
                    )
                except Exception as e:
                    if "1210" in str(e):
                        # ç­–ç•¥2ï¼šå®Œå…¨æœ€å°å‚æ•°ï¼ˆè¿max_tokensä¹Ÿå»æ‰ï¼‰
                        response = self.client.chat.completions.create(
                            model=model,
                            messages=messages,
                            stream=False
                        )
                    else:
                        raise e
            else:
                # æœªçŸ¥æ¨¡å‹ï¼šä½¿ç”¨åŸºç¡€å‚æ•°
                response = self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    stream=False
                )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            if response.choices and len(response.choices) > 0:
                generated_text = response.choices[0].message.content
                
                # æ›´æ–°å¯¹è¯å†å²
                self._update_conversation_history(text_input, image_base64, generated_text, history)
                
                # è·å–tokenä½¿ç”¨ä¿¡æ¯
                token_count = getattr(response.usage, 'total_tokens', 0) if hasattr(response, 'usage') else 0
                
                # ç”Ÿæˆå¯¹è¯ä¿¡æ¯
                conversation_info = self._generate_conversation_info(model, len(messages), token_count)
                
                return (generated_text, conversation_info, token_count)
            else:
                error_msg = "é”™è¯¯ï¼šAPIè¿”å›ç©ºå“åº”"
                return (error_msg, "APIå“åº”å¼‚å¸¸", 0)
                
        except ImportError as e:
            error_msg = f"å¯¼å…¥é”™è¯¯: {str(e)}"
            return (error_msg, "æ¨¡å—å¯¼å…¥å¤±è´¥", 0)
        except ConnectionError as e:
            error_msg = f"ç½‘ç»œè¿æ¥é”™è¯¯: {str(e)}"
            return (error_msg, "ç½‘ç»œå¼‚å¸¸", 0)
        except TimeoutError as e:
            error_msg = f"è¯·æ±‚è¶…æ—¶: {str(e)}"
            return (error_msg, "è¯·æ±‚è¶…æ—¶", 0)
        except ValueError as e:
            error_msg = f"å‚æ•°é”™è¯¯: {str(e)}"
            return (error_msg, "å‚æ•°å¼‚å¸¸", 0)
        except Exception as e:
            # ä¸¥æ ¼æŒ‰ç…§å®˜æ–¹æ–‡æ¡£å¤„ç†é”™è¯¯ï¼ˆä¸åšç‰¹æ®Šå‚æ•°å°è¯•ï¼‰
            error_detail = str(e)
            if "api_key" in error_detail.lower():
                error_msg = f"APIå¯†é’¥é”™è¯¯: {error_detail}"
                return (error_msg, "è®¤è¯å¤±è´¥", 0)
            elif "quota" in error_detail.lower() or "balance" in error_detail.lower():
                error_msg = f"ä½™é¢ä¸è¶³: {error_detail}"
                return (error_msg, "é…é¢ä¸è¶³", 0)
            elif "rate" in error_detail.lower():
                error_msg = f"è¯·æ±‚é¢‘ç‡è¿‡é«˜: {error_detail}"
                return (error_msg, "é¢‘ç‡é™åˆ¶", 0)
            elif "1210" in error_detail:
                error_msg = f"APIå‚æ•°é”™è¯¯(1210): {error_detail} - è¯·æ£€æŸ¥æ¨¡å‹åç§°å’Œå‚æ•°æ˜¯å¦ç¬¦åˆå®˜æ–¹æ–‡æ¡£"
                return (error_msg, "å‚æ•°ä¸å…¼å®¹", 0)
            else:
                error_msg = f"GLM Vision APIè°ƒç”¨å¤±è´¥: {error_detail}"
                return (error_msg, f"é”™è¯¯: {type(e).__name__}", 0)
    
    def _process_image(self, image: torch.Tensor, quality: str = "auto") -> Optional[str]:
        """å¤„ç†å›¾åƒå¹¶è½¬æ¢ä¸ºbase64ç¼–ç """
        try:
            # è½¬æ¢tensoråˆ°PIL Image
            if len(image.shape) == 4:
                # æ‰¹æ¬¡ç»´åº¦ï¼Œå–ç¬¬ä¸€å¼ å›¾ç‰‡
                image = image[0]
            
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            if image.dtype == torch.float32 or image.dtype == torch.float64:
                # å‡è®¾æ•°æ®èŒƒå›´æ˜¯[0, 1]
                image_np = (image.cpu().numpy() * 255).astype(np.uint8)
            else:
                image_np = image.cpu().numpy()
            
            # è½¬æ¢ä¸ºPILå›¾åƒ
            if len(image_np.shape) == 3:
                if image_np.shape[2] == 3:  # RGB
                    pil_image = Image.fromarray(image_np, 'RGB')
                elif image_np.shape[2] == 4:  # RGBA
                    pil_image = Image.fromarray(image_np, 'RGBA')
                    # è½¬æ¢ä¸ºRGB
                    pil_image = pil_image.convert('RGB')
                else:
                    return None
            else:
                return None
            
            # æ ¹æ®è´¨é‡è®¾ç½®è°ƒæ•´å›¾åƒ
            if quality == "low":
                # é™ä½åˆ†è¾¨ç‡
                max_size = (512, 512)
                pil_image.thumbnail(max_size, Image.Resampling.LANCZOS)
                jpeg_quality = 60
            elif quality == "high":
                # ä¿æŒé«˜åˆ†è¾¨ç‡
                max_size = (2048, 2048)
                if pil_image.size[0] > max_size[0] or pil_image.size[1] > max_size[1]:
                    pil_image.thumbnail(max_size, Image.Resampling.LANCZOS)
                jpeg_quality = 95
            else:  # auto
                # è‡ªåŠ¨è°ƒæ•´
                max_size = (1024, 1024)
                if pil_image.size[0] > max_size[0] or pil_image.size[1] > max_size[1]:
                    pil_image.thumbnail(max_size, Image.Resampling.LANCZOS)
                jpeg_quality = 85
            
            # è½¬æ¢ä¸ºbase64
            buffer = io.BytesIO()
            pil_image.save(buffer, format='JPEG', quality=jpeg_quality)
            image_bytes = buffer.getvalue()
            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
            
            return image_base64
            
        except Exception as e:
            pass
            return None
    
    def _build_messages(self, text_input: str, image_base64: str, history: int) -> List[Dict[str, Any]]:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆä¸¥æ ¼éµå¾ªæ™ºè°±AIå®˜æ–¹APIæ–‡æ¡£æ ¼å¼ï¼‰"""
        messages = []
        
        # æ·»åŠ å†å²å¯¹è¯ï¼ˆé™åˆ¶æ•°é‡ï¼‰
        history_count = min(len(self.conversation_history), history * 2)
        if history_count > 0:
            messages.extend(self.conversation_history[-history_count:])
        
        # æ„å»ºå½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹æ–‡æ¡£è§„èŒƒï¼‰
        # å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼šhttps://docs.bigmodel.cn/api-reference/æ¨¡å‹-api/å¯¹è¯è¡¥å…¨#è§†è§‰æ¨¡å‹
        current_message = {
            "role": "user",
            "content": [
                {
                    "type": "image_url", 
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{image_base64}"
                    }
                },
                {
                    "type": "text",
                    "text": text_input
                }
            ]
        }
        
        messages.append(current_message)
        return messages
    
    def _update_conversation_history(self, user_input: str, image_base64: str, 
                                   assistant_response: str, max_history: int):
        """æ›´æ–°å¯¹è¯å†å²ï¼ˆç¬¦åˆå®˜æ–¹APIæ ¼å¼ï¼‰"""
        # æ·»åŠ ç”¨æˆ·è¾“å…¥ï¼ˆä¸ºäº†èŠ‚çœå­˜å‚¨ï¼Œåªä¿å­˜æ–‡æœ¬éƒ¨åˆ†ï¼‰
        user_message = {
            "role": "user", 
            "content": user_input  # ç®€åŒ–ä¸ºçº¯æ–‡æœ¬ï¼Œé¿å…é‡å¤å­˜å‚¨å›¾åƒ
        }
        
        # æ·»åŠ åŠ©æ‰‹å›å¤
        assistant_message = {
            "role": "assistant", 
            "content": assistant_response
        }
        
        self.conversation_history.append(user_message)
        self.conversation_history.append(assistant_message)
        
        # é™åˆ¶å†å²é•¿åº¦
        max_messages = max_history * 2
        if len(self.conversation_history) > max_messages:
            self.conversation_history = self.conversation_history[-max_messages:]
    
    def _generate_conversation_info(self, model: str, message_count: int, token_count: int) -> str:
        """ç”Ÿæˆå¯¹è¯ä¿¡æ¯"""
        history_rounds = len(self.conversation_history) // 2
        info_lines = [
            f"æ¨¡å‹: {model}",
            f"æœ¬æ¬¡æ¶ˆæ¯æ•°: {message_count}",
            f"å†å²è½®æ•°: {history_rounds}",
            f"Tokenä½¿ç”¨: {token_count}",
            f"æ¨¡å¼: è§†è§‰+æ–‡æœ¬",
            f"å¯¹è¯çŠ¶æ€: æ­£å¸¸"
        ]
        return "\n".join(info_lines)
    
    @classmethod
    def IS_CHANGED(cls, **kwargs):
        # æ¯æ¬¡éƒ½é‡æ–°æ‰§è¡Œï¼Œå› ä¸ºæ˜¯APIè°ƒç”¨
        return float("nan")


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "GLMVisionAPI": GLMVisionAPI
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GLMVisionAPI": "GLM_è§†è§‰ä¸¨API"
}
