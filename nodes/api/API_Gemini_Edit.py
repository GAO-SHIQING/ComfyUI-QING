# -*- coding: utf-8 -*-
"""
Gemini Edit APIèŠ‚ç‚¹ - åŸºäºé€šç”¨æ¡†æ¶
è°ƒç”¨Geminiç¼–è¾‘æ¨¡å‹APIè¿›è¡Œå›¾åƒç¼–è¾‘å’Œç”Ÿæˆ
æ”¯æŒGoogle AI Studioå¹³å°
"""

from typing import Dict, Any
from .base_api_framework import (
    BaseVisionAPINode, 
    BasePlatformAdapter, 
    PlatformConfig
)


class GoogleAIStudioGeminiEditAdapter(BasePlatformAdapter):
    """Google AI Studio Geminiç¼–è¾‘å¹³å°é€‚é…å™¨"""
    
    def prepare_api_params(self, base_params: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        params = base_params.copy()
        
        # Geminiç¼–è¾‘æ¨¡å‹ä¸æ”¯æŒé‡å¤æƒ©ç½šå‚æ•°ï¼Œä½†ä¿æŒå…¼å®¹æ€§
        repetition_penalty = kwargs.get('repetition_penalty', 1.0)
        if repetition_penalty != 1.0:
            # Geminiä½¿ç”¨ä¸åŒçš„å‚æ•°åï¼Œè½¬æ¢ä¸ºtemperatureè°ƒæ•´
            current_temp = params.get('temperature', 0.4)
            adjusted_temp = max(0.0, min(2.0, current_temp + (repetition_penalty - 1.0) * 0.3))
            params['temperature'] = adjusted_temp
        
        # åº”ç”¨tokené™åˆ¶
        params["max_tokens"] = self.apply_token_limit(params["max_tokens"])
        
        # æ³¨æ„ï¼šimage_quality å‚æ•°ä»…ç”¨äºå†…éƒ¨é€»è¾‘ï¼Œä¸ä¼ é€’ç»™OpenAIå…¼å®¹API
        # OpenAIå…¼å®¹æ¥å£ä¸æ”¯æŒ image_quality å‚æ•°
        image_quality = kwargs.get('image_quality', 'auto')
        # å¯ä»¥åœ¨è¿™é‡Œæ ¹æ® image_quality è°ƒæ•´å…¶ä»–å‚æ•°ï¼Œä½†ä¸æ·»åŠ åˆ° params ä¸­
        
        # æ ¹æ®æ–‡æ¡£ï¼ŒGemini APIé€šè¿‡OpenAIå…¼å®¹å±‚æ”¯æŒæ€è€ƒåŠŸèƒ½
        # å¯ä»¥æ·»åŠ reasoning_effortå‚æ•°ï¼ˆlow, medium, highï¼‰
        reasoning_effort = kwargs.get('reasoning_effort', None)
        if reasoning_effort and reasoning_effort in ['low', 'medium', 'high']:
            params["reasoning_effort"] = reasoning_effort
        
        return params


class GeminiEditAPI(BaseVisionAPINode):
    """
    Geminiç¼–è¾‘æ¨¡å‹APIè°ƒç”¨èŠ‚ç‚¹ - æ¡†æ¶ç‰ˆæœ¬
    æ”¯æŒGoogle AI Studioå¹³å°çš„Geminiç¼–è¾‘æ¨¡å‹è°ƒç”¨
    """
    
    # èŠ‚ç‚¹ä¿¡æ¯
    NODE_NAME = "GeminiEditAPI"
    DISPLAY_NAME = "Gemini_ç¼–è¾‘ä¸¨API"
    
    # è¿”å›ç±»å‹
    RETURN_TYPES = ("IMAGE", "STRING", "INT")
    
    # è¿”å›åç§°
    RETURN_NAMES = ("edited_image", "conversation_info", "total_tokens")
    
    # Geminiç¼–è¾‘æ¨¡å‹åˆ—è¡¨
    GEMINI_EDIT_MODELS = [
        "gemini-2.5-flash-image-preview"
    ]
    
    # å¹³å°é…ç½®
    PLATFORM_CONFIGS = {
        "Google AI Studio": PlatformConfig(
            name="Google AI Studio",
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
            api_key_env="GEMINI_API_KEY",
            config_key="gemini_api_key",
            platform_key="google_ai_studio_edit",
            models=GEMINI_EDIT_MODELS,
            model_mapping={
                "gemini-2.5-flash-image-preview": "gemini-2.5-flash-image-preview"
            },
            max_tokens_limit=4096,  # å›¾åƒç¼–è¾‘æ¨¡å‹é€šå¸¸ä¸éœ€è¦è¿‡å¤štoken
            supports_frequency_penalty=False,
            # é’ˆå¯¹å›¾åƒç¼–è¾‘æ¨¡å‹çš„ç‰¹æ®Šé…ç½®ï¼Œä½¿ç”¨custom_paramså­˜å‚¨
            custom_params={
                "supports_image_editing": True,
                "supports_multi_image": True,
                "max_images": 3
            }
        )
    }
    
    # å¹³å°é€‚é…å™¨æ˜ å°„
    PLATFORM_ADAPTERS = {
        "Google AI Studio": GoogleAIStudioGeminiEditAdapter
    }
    
    @classmethod
    def get_input_types(cls) -> Dict[str, Any]:
        """é‡å†™è¾“å…¥ç±»å‹ä»¥æ·»åŠ Geminiç¼–è¾‘ç‰¹å®šçš„æç¤ºä¿¡æ¯å’Œå¤šå›¾åƒè¾“å…¥"""
        # ç”±äºçˆ¶ç±»BaseVisionAPINodeçš„get_input_typesä¼šè®¿é—®cls.PLATFORM_CONFIGS
        # è€Œåœ¨ç±»å®šä¹‰æ—¶è¿™å¯èƒ½è¿˜æœªå®Œå…¨åˆå§‹åŒ–ï¼Œæˆ‘ä»¬ç›´æ¥æ„å»ºè¾“å…¥ç±»å‹
        
        base_types = {
            "required": {
                "text_input": ("STRING", {
                    "default": "è¯·ç¼–è¾‘è¿™å¼ å›¾ç‰‡ã€‚",
                    "multiline": True,
                    "tooltip": "è¾“å…¥å›¾åƒç¼–è¾‘æŒ‡ä»¤ï¼Œæè¿°ä½ æƒ³è¦å¦‚ä½•ä¿®æ”¹æˆ–ç”Ÿæˆå›¾åƒã€‚å»ºè®®ä½¿ç”¨ç®€æ´æ˜ç¡®çš„æŒ‡ä»¤ï¼Œå¦‚'å°†èƒŒæ™¯æ”¹ä¸ºè“å¤©'ã€'è½¬æ¢ä¸ºæ°´å½©ç”»é£æ ¼'ç­‰"
                }),
                "platform": (list(cls.PLATFORM_CONFIGS.keys()), {
                    "default": list(cls.PLATFORM_CONFIGS.keys())[0] if cls.PLATFORM_CONFIGS else "Google AI Studio",
                    "tooltip": "é€‰æ‹©Geminiç¼–è¾‘æ¨¡å‹çš„æœåŠ¡å¹³å°"
                }),
                "model": (tuple(cls.GEMINI_EDIT_MODELS), {
                    "default": cls.GEMINI_EDIT_MODELS[0] if cls.GEMINI_EDIT_MODELS else "gemini-2.5-flash-image-preview",
                    "tooltip": """é€‰æ‹©è¦ä½¿ç”¨çš„Geminiç¼–è¾‘æ¨¡å‹
ğŸ“‹ gemini-2.5-flash-image-previewç‰¹ç‚¹ï¼š
ğŸ¨ ä¸“ä¸šå›¾åƒç¼–è¾‘ï¼šæ”¯æŒç²¾ç¡®çš„å›¾åƒä¿®æ”¹å’Œç”Ÿæˆ
ğŸ–¼ï¸ å¤šå›¾æ”¯æŒï¼šå¯åŒæ—¶å¤„ç†æœ€å¤š3å¼ å›¾åƒ
ğŸ­ é£æ ¼è½¬æ¢ï¼šæ“…é•¿è‰ºæœ¯é£æ ¼ã€æ»¤é•œæ•ˆæœè½¬æ¢
âœ¨ å†…å®¹ç¼–è¾‘ï¼šæ™ºèƒ½æ·»åŠ ã€åˆ é™¤æˆ–ä¿®æ”¹å›¾åƒå…ƒç´ 
âš¡ å¿«é€Ÿå“åº”ï¼šé’ˆå¯¹å›¾åƒç¼–è¾‘ä»»åŠ¡ä¼˜åŒ–çš„é«˜æ•ˆæ¨¡å‹
ğŸ’¡ æœ€ä½³å®è·µï¼šä½¿ç”¨è¾ƒä½temperature(0.2)ç¡®ä¿ç¼–è¾‘ä¸€è‡´æ€§"""
                }),
                "max_tokens": ("INT", {
                    "default": 2048,
                    "min": 1,
                    "max": 4096,
                    "step": 1,
                    "tooltip": "æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶æœ€å¤šèƒ½ä½¿ç”¨çš„tokenæ•°é‡ï¼ˆ1-4096ï¼Œå›¾åƒç¼–è¾‘ä¼˜åŒ–ï¼‰"
                }),
                "history": ("INT", {
                    "default": 4,
                    "min": 1,
                    "max": 18,
                    "step": 1,
                    "tooltip": "ä¿æŒçš„å†å²å¯¹è¯è½®æ•°ï¼ˆ1-18è½®ï¼Œç¼–è¾‘ä»»åŠ¡å»ºè®®4è½®ï¼‰"
                }),
            },
            "optional": {
                "temperature": ("FLOAT", {
                    "default": 0.2,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.01,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆç»“æœçš„éšæœºæ€§ï¼Œè¶Šé«˜è¶Šéšæœºï¼ˆ0.0-2.0ï¼Œç¼–è¾‘å»ºè®®0.2ç¡®ä¿ä¸€è‡´æ€§ï¼‰"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.8,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.01,
                    "tooltip": "æ§åˆ¶ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§ï¼ˆ0.0-1.0ï¼‰"
                }),
                "image_quality": (["auto", "low", "high"], {
                    "default": "high",
                    "tooltip": "å›¾åƒå¤„ç†è´¨é‡ï¼šauto(è‡ªåŠ¨é€‰æ‹©), low(ä½è´¨é‡ï¼Œé€Ÿåº¦å¿«), high(é«˜è´¨é‡ï¼Œç²¾åº¦é«˜)"
                }),
                "reasoning_effort": (["none", "low", "medium", "high"], {
                    "default": "medium",
                    "tooltip": "æ¨ç†åŠªåŠ›ç¨‹åº¦ï¼šnone(æ— ), low(ä½), medium(ä¸­ç­‰), high(é«˜) - å½±å“ç¼–è¾‘è´¨é‡å’Œå¤„ç†æ—¶é—´"
                }),
                "clear_history": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "æ˜¯å¦æ¸…é™¤å†å²å¯¹è¯è®°å½•"
                })
            }
        }
        
        # æ·»åŠ ä¸‰ä¸ªå›¾åƒè¾“å…¥ç«¯å£
        base_types["required"]["image1"] = ("IMAGE", {
            "tooltip": "è¾“å…¥è¦ç¼–è¾‘çš„ç¬¬ä¸€å¼ å›¾åƒï¼ˆå¿…éœ€ï¼‰"
        })
        
        base_types["optional"]["image2"] = ("IMAGE", {
            "tooltip": "è¾“å…¥è¦ç¼–è¾‘çš„ç¬¬äºŒå¼ å›¾åƒï¼ˆå¯é€‰ï¼‰"
        })
        
        base_types["optional"]["image3"] = ("IMAGE", {
            "tooltip": "è¾“å…¥è¦ç¼–è¾‘çš„ç¬¬ä¸‰å¼ å›¾åƒï¼ˆå¯é€‰ï¼‰"
        })
        
        return base_types
    
    def generate_response(self, text_input: str, image1, image2=None, image3=None, platform: str = "Google AI Studio", 
                         model: str = "gemini-2.5-flash-image-preview", max_tokens: int = 2048, 
                         history: int = 4, temperature: float = 0.2, top_p: float = 0.8,
                         image_quality: str = "high", reasoning_effort: str = "medium",
                         clear_history: bool = False, **kwargs) -> tuple:
        """ç”Ÿæˆå“åº”ï¼Œæ”¯æŒå¤šå›¾åƒè¾“å…¥"""
        
        # æ”¶é›†æ‰€æœ‰éç©ºçš„å›¾åƒ
        images = []
        if image1 is not None:
            images.append(image1)
        if image2 is not None:
            images.append(image2)
        if image3 is not None:
            images.append(image3)
        
        # è‡³å°‘éœ€è¦ä¸€å¼ å›¾åƒ
        if not images:
            raise ValueError("è‡³å°‘éœ€è¦æä¾›ä¸€å¼ å›¾åƒè¿›è¡Œç¼–è¾‘")
        
        try:
            # æ¸…é™¤å†å²è®°å½•
            if clear_history:
                self.conversation_manager.clear_all()
            
            # éªŒè¯è¾“å…¥å‚æ•°
            is_valid, error_msg = self._validate_inputs(platform, model)
            if not is_valid:
                return (None, error_msg, 0)
            
            # è·å–å¹³å°é…ç½®å’Œé€‚é…å™¨
            platform_config = self.PLATFORM_CONFIGS[platform]
            adapter = self._get_platform_adapter(platform)
            
            # è·å–APIå¯†é’¥
            api_key = self.api_key_manager.get_api_key(platform_config)
            if not api_key:
                error_msg = f"""é”™è¯¯ï¼šæœªæä¾›{platform}çš„APIå¯†é’¥ã€‚è¯·å°è¯•ä»¥ä¸‹æ–¹æ³•ä¹‹ä¸€ï¼š

1. è®¾ç½®ç¯å¢ƒå˜é‡ï¼š{platform_config.api_key_env}
2. åœ¨ğŸ¨QINGè®¾ç½®ä¸­é…ç½®APIå¯†é’¥
3. ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è®¾ç½®å¯†é’¥

è·å–Gemini APIå¯†é’¥ï¼šhttps://aistudio.google.com/app/apikey

è¯·é…ç½®åé‡è¯•ã€‚"""
                return (None, error_msg, 0)
            
            # è·å–æˆ–åˆ›å»ºå®¢æˆ·ç«¯
            client = self.client_manager.get_or_create_client(platform_config, api_key)
            
            # ç®¡ç†å¯¹è¯å†å²
            conversation_key = self.conversation_manager.get_conversation_key(
                platform_config.platform_key, model
            )
            
            # å‡†å¤‡å¤šå›¾åƒæ¶ˆæ¯
            messages = self._prepare_edit_messages(images, text_input, conversation_key)
            
            # é™åˆ¶å†å²é•¿åº¦
            self.conversation_manager.limit_history(conversation_key, history)
            
            # å‡†å¤‡åŸºç¡€APIå‚æ•°
            base_params = self._prepare_base_params(
                adapter.get_api_model_name(model),
                messages,
                adapter.apply_token_limit(max_tokens),
                temperature,
                top_p
            )
            
            # ä½¿ç”¨é€‚é…å™¨å‡†å¤‡æœ€ç»ˆå‚æ•°ï¼ˆåŒ…å«å›¾åƒè´¨é‡å’Œreasoning_effortï¼‰
            api_params = adapter.prepare_api_params(
                base_params, 
                model=model, 
                image_quality=image_quality,
                reasoning_effort=reasoning_effort,
                additional_images=images[1:] if len(images) > 1 else []
            )
            
            # è°ƒç”¨API
            response = client.chat.completions.create(**api_params)
            
            # å¤„ç†å“åº”
            reply, usage_info = self._handle_api_response(response, conversation_key)
            
            # å¯¹äºå›¾åƒç¼–è¾‘APIï¼ŒGeminiè¿”å›çš„æ˜¯å®é™…å›¾åƒæ•°æ®
            # è§£æè¿”å›çš„å›¾åƒï¼ˆé€šå¸¸åœ¨responseä¸­æˆ–ä½œä¸ºURLï¼‰
            edited_image = self._parse_edited_image(response, reply)
            
            # ç”Ÿæˆå¯¹è¯ä¿¡æ¯ï¼ŒåŒ…å«reasoning_effort
            reasoning_info = f" | æ¨ç†: {reasoning_effort}" if reasoning_effort != "none" else ""
            conversation_info = f"å¹³å°: {platform} | æ¨¡å‹: {model} | å†å²è½®æ•°: {len(messages)//2} | å›¾åƒè´¨é‡: {image_quality}{reasoning_info} | æ€»Tokens: {usage_info['total_tokens']} (è¾“å…¥: {usage_info['prompt_tokens']}, è¾“å‡º: {usage_info['completion_tokens']}, é™åˆ¶: {max_tokens})"
            
            return (edited_image, conversation_info, usage_info['total_tokens'])
            
        except Exception as e:
            error_message = f"{self.NODE_NAME} APIè°ƒç”¨å¤±è´¥ ({platform}): {str(e)}"
            return (None, error_message, 0)
    
    def _prepare_edit_messages(self, images: list, text_input: str, conversation_key: str) -> list:
        """å‡†å¤‡å›¾åƒç¼–è¾‘æ¨¡å‹çš„æ¶ˆæ¯æ ¼å¼ï¼Œæ”¯æŒå¤šå›¾åƒ"""
        content_parts = []
        
        # æ·»åŠ æ‰€æœ‰å›¾åƒ
        for i, image in enumerate(images):
            image_base64 = self._image_to_base64(image)
            content_parts.append({
                "type": "image_url",
                "image_url": {
                    "url": image_base64
                }
            })
        
        # æ·»åŠ æ–‡æœ¬æŒ‡ä»¤
        content_parts.append({
            "type": "text",
            "text": text_input
        })
        
        # åˆ›å»ºç”¨æˆ·æ¶ˆæ¯
        user_message = {
            "role": "user",
            "content": content_parts
        }
        
        # æ·»åŠ åˆ°å¯¹è¯å†å²
        self.conversation_manager.add_message(conversation_key, "user", user_message["content"])
        
        return self.conversation_manager.get_history(conversation_key)
    
    def _parse_edited_image(self, response, reply: str):
        """
        è§£æGeminiå›¾åƒç¼–è¾‘APIè¿”å›çš„å®é™…å›¾åƒæ•°æ®
        gemini-2.5-flash-image-previewæ¨¡å‹è¿”å›çš„æ˜¯å®é™…å›¾åƒ
        """
        import torch
        import numpy as np
        from PIL import Image
        import base64
        from io import BytesIO
        import re
        
        try:
            # æ–¹æ³•1: å°è¯•ä»response.choicesä¸­æå–å›¾åƒURLæˆ–base64æ•°æ®
            if hasattr(response, 'choices') and len(response.choices) > 0:
                choice = response.choices[0]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒå†…å®¹
                if hasattr(choice.message, 'content'):
                    content = choice.message.content
                    
                    # å¦‚æœcontentæ˜¯åˆ—è¡¨ï¼ˆå¤šæ¨¡æ€å“åº”ï¼‰
                    if isinstance(content, list):
                        for item in content:
                            if isinstance(item, dict):
                                # æ£€æŸ¥å›¾åƒURL
                                if item.get('type') == 'image_url':
                                    image_url = item.get('image_url', {}).get('url', '')
                                    if image_url:
                                        return self._load_image_from_url_or_base64(image_url)
                                
                                # æ£€æŸ¥ç›´æ¥çš„å›¾åƒæ•°æ®
                                if 'image' in item or 'image_data' in item:
                                    image_data = item.get('image') or item.get('image_data')
                                    return self._load_image_from_base64(image_data)
                    
                    # å¦‚æœcontentæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•æŸ¥æ‰¾base64å›¾åƒæ•°æ®
                    elif isinstance(content, str):
                        # æŸ¥æ‰¾base64å›¾åƒæ•°æ®æ¨¡å¼
                        base64_pattern = r'data:image/[^;]+;base64,([^"\s]+)'
                        matches = re.findall(base64_pattern, content)
                        if matches:
                            return self._load_image_from_base64(matches[0])
                        
                        # å°è¯•ç›´æ¥ä½œä¸ºbase64è§£ç 
                        if len(content) > 100 and not content.startswith('http'):
                            try:
                                return self._load_image_from_base64(content)
                            except:
                                pass
            
            # æ–¹æ³•2: æ£€æŸ¥responseä¸­æ˜¯å¦æœ‰ç›´æ¥çš„å›¾åƒå­—æ®µ
            if hasattr(response, 'data'):
                if isinstance(response.data, list):
                    for item in response.data:
                        if hasattr(item, 'url'):
                            return self._load_image_from_url_or_base64(item.url)
                        if hasattr(item, 'b64_json'):
                            return self._load_image_from_base64(item.b64_json)
            
            # æ–¹æ³•3: å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›æç¤ºå›¾åƒ
            return self._create_error_image("æ— æ³•è§£æå›¾åƒæ•°æ®ï¼Œè¯·æ£€æŸ¥APIå“åº”æ ¼å¼")
            
        except Exception as e:
            return self._create_error_image(f"å›¾åƒè§£æå¤±è´¥: {str(e)}")
    
    def _load_image_from_url_or_base64(self, data: str):
        """
        ä»URLæˆ–base64å­—ç¬¦ä¸²åŠ è½½å›¾åƒ
        """
        import torch
        import numpy as np
        from PIL import Image
        import base64
        from io import BytesIO
        import requests
        
        try:
            # å¦‚æœæ˜¯base64æ•°æ®URI
            if data.startswith('data:image'):
                # æå–base64éƒ¨åˆ†
                base64_data = data.split(',', 1)[1] if ',' in data else data
                return self._load_image_from_base64(base64_data)
            
            # å¦‚æœæ˜¯HTTP(S) URL
            elif data.startswith('http://') or data.startswith('https://'):
                response = requests.get(data, timeout=30)
                response.raise_for_status()
                img = Image.open(BytesIO(response.content))
                return self._pil_to_tensor(img)
            
            # å¦åˆ™å°è¯•ä½œä¸ºbase64
            else:
                return self._load_image_from_base64(data)
                
        except Exception as e:
            raise ValueError(f"åŠ è½½å›¾åƒå¤±è´¥: {str(e)}")
    
    def _load_image_from_base64(self, base64_str: str):
        """
        ä»base64å­—ç¬¦ä¸²åŠ è½½å›¾åƒ
        """
        import torch
        import numpy as np
        from PIL import Image
        import base64
        from io import BytesIO
        
        try:
            # æ¸…ç†base64å­—ç¬¦ä¸²
            base64_str = base64_str.strip().replace('\n', '').replace('\r', '')
            
            # è§£ç base64
            image_data = base64.b64decode(base64_str)
            
            # åŠ è½½å›¾åƒ
            img = Image.open(BytesIO(image_data))
            
            return self._pil_to_tensor(img)
            
        except Exception as e:
            raise ValueError(f"Base64è§£ç å¤±è´¥: {str(e)}")
    
    def _pil_to_tensor(self, img: 'Image'):
        """
        å°†PILå›¾åƒè½¬æ¢ä¸ºComfyUI tensoræ ¼å¼
        """
        import torch
        import numpy as np
        
        # è½¬æ¢ä¸ºRGBï¼ˆå¦‚æœæ˜¯RGBAæˆ–å…¶ä»–æ ¼å¼ï¼‰
        if img.mode != 'RGB':
            img = img.convert('RGB')
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å½’ä¸€åŒ–åˆ°0-1
        img_np = np.array(img).astype(np.float32) / 255.0
        
        # è½¬æ¢ä¸ºtorch tensorï¼Œæ·»åŠ batchç»´åº¦ [1, H, W, C]
        img_tensor = torch.from_numpy(img_np)[None,]
        
        return img_tensor
    
    def _create_error_image(self, error_message: str):
        """
        åˆ›å»ºä¸€ä¸ªé”™è¯¯æç¤ºå›¾åƒ
        """
        import torch
        import numpy as np
        from PIL import Image, ImageDraw, ImageFont
        
        try:
            # åˆ›å»º512x512çš„æµ…çº¢è‰²èƒŒæ™¯å›¾åƒ
            img = Image.new('RGB', (512, 512), color=(255, 230, 230))
            draw = ImageDraw.Draw(img)
            
            # å°è¯•ä½¿ç”¨å­—ä½“
            try:
                font = ImageFont.truetype("arial.ttf", 16)
            except:
                font = ImageFont.load_default()
            
            # ç»˜åˆ¶é”™è¯¯æ¶ˆæ¯
            lines = ["âš ï¸ å›¾åƒè§£æé”™è¯¯", "", error_message[:50]]
            y_offset = 200
            for line in lines:
                bbox = draw.textbbox((0, 0), line, font=font)
                text_width = bbox[2] - bbox[0]
                x = (512 - text_width) // 2
                draw.text((x, y_offset), line, fill='red', font=font)
                y_offset += 30
            
            # è½¬æ¢ä¸ºtensor
            img_np = np.array(img).astype(np.float32) / 255.0
            return torch.from_numpy(img_np)[None,]
            
        except Exception:
            # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼šçº¯è‰²å›¾åƒ
            img_np = np.full((512, 512, 3), 0.9, dtype=np.float32)
            return torch.from_numpy(img_np)[None,]


# ComfyUIèŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "GeminiEditAPI": GeminiEditAPI
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GeminiEditAPI": "Gemini_ç¼–è¾‘ä¸¨API"
}
