# -*- coding: utf-8 -*-
import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
import math
import comfy.utils


class Imagesizescaling:
    """
    å›¾åƒç¼©æ”¾èŠ‚ç‚¹ - å®ç°å›¾åƒå’Œé®ç½©çš„é«˜çº§ç¼©æ”¾åŠŸèƒ½
    
    åŠŸèƒ½:
    - åŒæ—¶å¤„ç†å›¾åƒå’Œé®ç½©çš„ç¼©æ”¾
    - æ”¯æŒå¤šç§ç¼©æ”¾æ–¹å¼ï¼šæ‹‰ä¼¸ã€è£å‰ªã€å¡«å……ã€ä¿æŒæ¯”ä¾‹
    - æ”¯æŒå¤šç§æ’å€¼æ–¹æ³•ï¼šnearestã€bilinearã€bicubicã€areaã€nearest-exactã€lanczos
    - æ”¯æŒå¤šç§ç¼©æ”¾å®šä¹‰ï¼šæ— ã€æœ€é•¿è¾¹ã€æœ€çŸ­è¾¹ã€å®½åº¦ã€é«˜åº¦ã€ç™¾åˆ†æ¯”ã€æ€»åƒç´ 
    - æ”¯æŒå›¾åƒå€æ•°çº¦æŸ
    - è¾“å‡ºæœ€ç»ˆçš„å›¾åƒã€é®ç½©å’Œå°ºå¯¸ä¿¡æ¯
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "scale_mode": (["keep_ratio", "stretch", "crop", "pad"], {
                    "default": "keep_ratio"
                }),
                "interpolation": (["lanczos", "nearest", "bilinear", "bicubic", "area", "nearest-exact"], {
                    "default": "lanczos"
                }),
                "scale_definition": (["none", "longest_side", "shortest_side", "width", "height", "percentage", "total_pixels"], {
                    "default": "longest_side"
                }),
                "definition_value": ("INT", {
                    "default": 1024,
                    "min": 1,
                    "max": 999999999,
                    "step": 1
                }),
                "multiple_of": ("INT", {
                    "default": 2,
                    "min": 1,
                    "max": 256,
                    "step": 1
                }),
            },
            "optional": {
                "image": ("IMAGE",),
                "mask": ("MASK",),
                "width": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 999999999,
                    "step": 1
                }),
                "height": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 999999999,
                    "step": 1
                }),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "MASK", "INT", "INT")
    RETURN_NAMES = ("image", "mask", "width", "height")
    FUNCTION = "scale_image"
    CATEGORY = "ğŸ¨QING/å›¾åƒå¤„ç†"
    
    def scale_image(self, scale_mode, interpolation, scale_definition, definition_value, multiple_of,
                   width=0, height=0, image=None, mask=None):
        """
        ä¸»å¤„ç†å‡½æ•°ï¼šå®ç°å›¾åƒå’Œé®ç½©çš„ç¼©æ”¾
        
        å‚æ•°:
            scale_mode: ç¼©æ”¾æ–¹å¼
            interpolation: æ’å€¼æ–¹æ³•
            scale_definition: ç¼©æ”¾å®šä¹‰
            definition_value: å®šä¹‰è°ƒæ•´å€¼
            multiple_of: å›¾åƒå€æ•°
            image: è¾“å…¥å›¾åƒï¼ˆå¯é€‰ï¼‰
            mask: è¾“å…¥é®ç½©ï¼ˆå¯é€‰ï¼‰
            width: ç›®æ ‡å®½åº¦ï¼ˆå¯é€‰ï¼‰
            height: ç›®æ ‡é«˜åº¦ï¼ˆå¯é€‰ï¼‰
            
        è¿”å›:
            tuple: (ç¼©æ”¾åçš„å›¾åƒ, ç¼©æ”¾åçš„é®ç½©, æœ€ç»ˆå®½åº¦, æœ€ç»ˆé«˜åº¦)
        """
        
        # éªŒè¯è¾“å…¥
        if image is None and mask is None:
            raise ValueError("è‡³å°‘éœ€è¦æä¾›å›¾åƒæˆ–é®ç½©ä¸­çš„ä¸€ä¸ª")
        
        # è·å–åŸå§‹å°ºå¯¸
        if image is not None:
            orig_height, orig_width = self._get_image_size(image)
        elif mask is not None:
            orig_height, orig_width = self._get_mask_size(mask)
        else:
            orig_height, orig_width = 512, 512  # é»˜è®¤å°ºå¯¸
        
        # è®¡ç®—ç›®æ ‡å°ºå¯¸
        target_width, target_height = self._calculate_target_size(
            orig_width, orig_height, width, height, scale_mode, 
            scale_definition, definition_value, multiple_of
        )
        
        # ç¼©æ”¾å›¾åƒ
        scaled_image = None
        if image is not None:
            scaled_image = self._scale_image_tensor(
                image, target_width, target_height, scale_mode, interpolation
            )
        else:
            # åˆ›å»ºç©ºå›¾åƒ
            scaled_image = self._create_empty_image(target_width, target_height)
        
        # ç¼©æ”¾é®ç½©
        scaled_mask = None
        if mask is not None:
            scaled_mask = self._scale_mask_tensor(
                mask, target_width, target_height, interpolation
            )
        else:
            # åˆ›å»ºç™½è‰²é®ç½©
            scaled_mask = self._create_white_mask(target_width, target_height)
        
        return (scaled_image, scaled_mask, target_width, target_height)
    
    def _get_image_size(self, image):
        """è·å–å›¾åƒå°ºå¯¸"""
        if image.dim() == 4:  # (B, H, W, C)
            return image.shape[1], image.shape[2]
        elif image.dim() == 3:  # (H, W, C)
            return image.shape[0], image.shape[1]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒç»´åº¦: {image.dim()}")
    
    def _get_mask_size(self, mask):
        """è·å–é®ç½©å°ºå¯¸"""
        if mask.dim() == 3:  # (B, H, W)
            return mask.shape[1], mask.shape[2]
        elif mask.dim() == 2:  # (H, W)
            return mask.shape[0], mask.shape[1]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é®ç½©ç»´åº¦: {mask.dim()}")
    
    def _calculate_target_size(self, orig_width, orig_height, width, height, 
                              scale_mode, scale_definition, definition_value, multiple_of):
        """è®¡ç®—ç›®æ ‡å°ºå¯¸"""
        
        # å¦‚æœç›´æ¥æŒ‡å®šäº†å®½é«˜ï¼Œä¼˜å…ˆä½¿ç”¨
        if width > 0 and height > 0:
            target_width, target_height = width, height
        elif width > 0:
            target_width = width
            if scale_mode == "pad":
                # padæ¨¡å¼ï¼šå¦‚æœåªæŒ‡å®šå®½åº¦ï¼Œé«˜åº¦ä¿æŒåŸå§‹å€¼ï¼ˆä¸ç¼©æ”¾ï¼‰
                target_height = orig_height
            elif scale_mode == "keep_ratio":
                target_height = max(1, int(orig_height * (width / orig_width)))
            else:
                target_height = orig_height
        elif height > 0:
            target_height = height
            if scale_mode == "pad":
                # padæ¨¡å¼ï¼šå¦‚æœåªæŒ‡å®šé«˜åº¦ï¼Œå®½åº¦ä¿æŒåŸå§‹å€¼ï¼ˆä¸ç¼©æ”¾ï¼‰
                target_width = orig_width
            elif scale_mode == "keep_ratio":
                target_width = max(1, int(orig_width * (height / orig_height)))
            else:
                target_width = orig_width
        else:
            # æ ¹æ®ç¼©æ”¾å®šä¹‰è®¡ç®—å°ºå¯¸
            target_width, target_height = self._calculate_by_definition(
                orig_width, orig_height, scale_definition, definition_value, scale_mode
            )
        
        # åº”ç”¨å€æ•°çº¦æŸ
        if multiple_of > 1:
            # ä½¿ç”¨å°±è¿‘èˆå…¥æ¨¡å¼ï¼Œå‡å°‘ä¸å¿…è¦çš„å°ºå¯¸å˜åŒ–
            target_width = round(target_width / multiple_of) * multiple_of
            target_height = round(target_height / multiple_of) * multiple_of
        
        # ç¡®ä¿æœ€å°å°ºå¯¸
        target_width = max(1, target_width)
        target_height = max(1, target_height)
        
        return target_width, target_height
    
    def _calculate_by_definition(self, orig_width, orig_height, scale_definition, 
                                definition_value, scale_mode):
        """æ ¹æ®ç¼©æ”¾å®šä¹‰è®¡ç®—ç›®æ ‡å°ºå¯¸"""
        
        if scale_definition == "none":
            return orig_width, orig_height
        
        elif scale_definition == "longest_side":
            if orig_width >= orig_height:
                target_width = definition_value
                if scale_mode in ["keep_ratio", "pad"]:
                    target_height = max(1, int(orig_height * (definition_value / orig_width)))
                else:
                    target_height = orig_height
            else:
                target_height = definition_value
                if scale_mode in ["keep_ratio", "pad"]:
                    target_width = max(1, int(orig_width * (definition_value / orig_height)))
                else:
                    target_width = orig_width
        
        elif scale_definition == "shortest_side":
            if orig_width <= orig_height:
                target_width = definition_value
                if scale_mode in ["keep_ratio", "pad"]:
                    target_height = max(1, int(orig_height * (definition_value / orig_width)))
                else:
                    target_height = orig_height
            else:
                target_height = definition_value
                if scale_mode in ["keep_ratio", "pad"]:
                    target_width = max(1, int(orig_width * (definition_value / orig_height)))
                else:
                    target_width = orig_width
        
        elif scale_definition == "width":
            target_width = definition_value
            if scale_mode in ["keep_ratio", "pad"]:
                target_height = max(1, int(orig_height * (definition_value / orig_width)))
            else:
                target_height = orig_height
        
        elif scale_definition == "height":
            target_height = definition_value
            if scale_mode in ["keep_ratio", "pad"]:
                target_width = max(1, int(orig_width * (definition_value / orig_height)))
            else:
                target_width = orig_width
        
        elif scale_definition == "percentage":
            scale_factor = definition_value / 100.0
            target_width = max(1, int(orig_width * scale_factor))
            target_height = max(1, int(orig_height * scale_factor))
        
        elif scale_definition == "total_pixels":
            if scale_mode in ["keep_ratio", "pad"]:
                aspect_ratio = orig_width / orig_height
                target_height = max(1, int(math.sqrt(definition_value / aspect_ratio)))
                target_width = max(1, int(target_height * aspect_ratio))
            else:
                # éä¿æŒæ¯”ä¾‹æ—¶ï¼Œå¹³å‡åˆ†é…åƒç´ 
                side_length = max(1, int(math.sqrt(definition_value)))
                target_width = side_length
                target_height = side_length
        
        else:
            target_width, target_height = orig_width, orig_height
        
        return target_width, target_height
    
    def _scale_image_tensor(self, image, target_width, target_height, scale_mode, interpolation):
        """ç¼©æ”¾å›¾åƒå¼ é‡"""
        
        # ç¡®ä¿å›¾åƒæ˜¯4ç»´ (B, H, W, C)
        if image.dim() == 3:
            image = image.unsqueeze(0)
        
        device = image.device
        dtype = image.dtype
        
        # è·å–åŸå§‹å°ºå¯¸
        batch_size, orig_height, orig_width, channels = image.shape
        
        if scale_mode == "stretch":
            # ç›´æ¥æ‹‰ä¼¸åˆ°ç›®æ ‡å°ºå¯¸
            return self._resize_image(image, target_width, target_height, interpolation)
        
        elif scale_mode == "keep_ratio":
            # ä¿æŒæ¯”ä¾‹ç¼©æ”¾ï¼Œå¯èƒ½ä¼šæœ‰é»‘è¾¹
            scale_x = target_width / orig_width
            scale_y = target_height / orig_height
            scale = min(scale_x, scale_y)
            
            new_width = int(orig_width * scale)
            new_height = int(orig_height * scale)
            
            # å…ˆç¼©æ”¾åˆ°åˆé€‚å¤§å°
            scaled = self._resize_image(image, new_width, new_height, interpolation)
            
            # ç„¶åå¡«å……åˆ°ç›®æ ‡å°ºå¯¸
            return self._pad_image(scaled, target_width, target_height)
        
        elif scale_mode == "crop":
            # ä¿æŒæ¯”ä¾‹ç¼©æ”¾ï¼Œè£å‰ªå¤šä½™éƒ¨åˆ†
            scale_x = target_width / orig_width
            scale_y = target_height / orig_height
            scale = max(scale_x, scale_y)
            
            new_width = int(orig_width * scale)
            new_height = int(orig_height * scale)
            
            # å…ˆç¼©æ”¾åˆ°åˆé€‚å¤§å°
            scaled = self._resize_image(image, new_width, new_height, interpolation)
            
            # ç„¶åå±…ä¸­è£å‰ª
            return self._center_crop_image(scaled, target_width, target_height)
        
        elif scale_mode == "pad":
            # padæ¨¡å¼ï¼šä¿æŒåŸå§‹å°ºå¯¸ï¼Œç›´æ¥å¡«å……åˆ°ç›®æ ‡å°ºå¯¸
            # ä¸è¿›è¡Œä»»ä½•ç¼©æ”¾ï¼Œç›´æ¥å°†åŸå›¾æ”¾ç½®åœ¨ç›®æ ‡ç”»å¸ƒä¸­å¿ƒ
            return self._pad_image(image, target_width, target_height)
        
        else:
            return self._resize_image(image, target_width, target_height, interpolation)
    
    def _scale_mask_tensor(self, mask, target_width, target_height, interpolation):
        """ç¼©æ”¾é®ç½©å¼ é‡"""
        
        # ç¡®ä¿é®ç½©æ˜¯3ç»´ (B, H, W)
        if mask.dim() == 2:
            mask = mask.unsqueeze(0)
        
        device = mask.device
        dtype = mask.dtype
        
        # é®ç½©å§‹ç»ˆä½¿ç”¨æœ€è¿‘é‚»æˆ–åŒçº¿æ€§æ’å€¼ï¼Œä¸ä½¿ç”¨å¤æ‚çš„æ’å€¼æ–¹æ³•
        if interpolation in ["nearest", "nearest-exact"]:
            mode = "nearest"
        else:
            mode = "bilinear"
        
        # è½¬æ¢ä¸ºfloatè¿›è¡Œæ’å€¼
        mask_float = mask.float().unsqueeze(1)  # (B, 1, H, W)
        
        # ä½¿ç”¨F.interpolateè¿›è¡Œç¼©æ”¾
        scaled_mask = F.interpolate(
            mask_float,
            size=(target_height, target_width),
            mode=mode,
            align_corners=False if mode != "nearest" else None
        ).squeeze(1)  # (B, H, W)
        
        # è½¬å›åŸå§‹æ•°æ®ç±»å‹å¹¶é™åˆ¶èŒƒå›´
        scaled_mask = torch.clamp(scaled_mask, 0.0, 1.0)
        if dtype != torch.float32:
            scaled_mask = scaled_mask.to(dtype)
        
        return scaled_mask
    
    def _resize_image(self, image, target_width, target_height, interpolation):
        """ä½¿ç”¨æŒ‡å®šæ’å€¼æ–¹æ³•ç¼©æ”¾å›¾åƒ"""
        
        device = image.device
        dtype = image.dtype
        
        if interpolation == "lanczos":
            # ä½¿ç”¨PILè¿›è¡ŒLanczosæ’å€¼
            return self._resize_image_pil(image, target_width, target_height, Image.LANCZOS)
        
        elif interpolation == "area":
            # ä½¿ç”¨PILè¿›è¡ŒåŒºåŸŸæ’å€¼ï¼ˆé€‚åˆç¼©å°ï¼‰
            return self._resize_image_pil(image, target_width, target_height, Image.BOX)
        
        elif interpolation == "nearest-exact":
            # ç²¾ç¡®æœ€è¿‘é‚»æ’å€¼
            return self._resize_image_pil(image, target_width, target_height, Image.NEAREST)
        
        else:
            # ä½¿ç”¨PyTorchçš„F.interpolate
            mode_map = {
                "nearest": "nearest",
                "bilinear": "bilinear",
                "bicubic": "bicubic"
            }
            mode = mode_map.get(interpolation, "bilinear")
            
            # è½¬æ¢ç»´åº¦ (B, H, W, C) -> (B, C, H, W)
            image_transposed = image.permute(0, 3, 1, 2)
            
            # ç¼©æ”¾
            scaled = F.interpolate(
                image_transposed,
                size=(target_height, target_width),
                mode=mode,
                align_corners=False if mode != "nearest" else None
            )
            
            # è½¬æ¢å› (B, H, W, C)
            return scaled.permute(0, 2, 3, 1)
    
    def _resize_image_pil(self, image, target_width, target_height, pil_method):
        """ä½¿ç”¨PILè¿›è¡Œå›¾åƒç¼©æ”¾"""
        
        device = image.device
        dtype = image.dtype
        batch_size = image.shape[0]
        
        scaled_images = []
        for i in range(batch_size):
            # è½¬æ¢ä¸ºPILå›¾åƒ
            img_np = (image[i].cpu().numpy() * 255).astype(np.uint8)
            if img_np.shape[2] == 1:
                pil_img = Image.fromarray(img_np[:, :, 0], mode='L')
            elif img_np.shape[2] == 3:
                pil_img = Image.fromarray(img_np, mode='RGB')
            elif img_np.shape[2] == 4:
                pil_img = Image.fromarray(img_np, mode='RGBA')
            else:
                # ä¸æ”¯æŒçš„é€šé“æ•°ï¼Œå–å‰3ä¸ªé€šé“
                pil_img = Image.fromarray(img_np[:, :, :3], mode='RGB')
            
            # ç¼©æ”¾
            resized_pil = pil_img.resize((target_width, target_height), pil_method)
            
            # è½¬æ¢å›å¼ é‡
            resized_np = np.array(resized_pil).astype(np.float32) / 255.0
            if resized_np.ndim == 2:
                resized_np = resized_np[:, :, np.newaxis]  # æ·»åŠ é€šé“ç»´åº¦
            elif resized_np.ndim == 3 and resized_np.shape[2] != image.shape[3]:
                # è°ƒæ•´é€šé“æ•°ä»¥åŒ¹é…åŸå§‹å›¾åƒ
                if image.shape[3] == 3 and resized_np.shape[2] == 4:
                    resized_np = resized_np[:, :, :3]  # ç§»é™¤alphaé€šé“
                elif image.shape[3] == 4 and resized_np.shape[2] == 3:
                    # æ·»åŠ alphaé€šé“
                    alpha = np.ones((resized_np.shape[0], resized_np.shape[1], 1), dtype=np.float32)
                    resized_np = np.concatenate([resized_np, alpha], axis=2)
                elif image.shape[3] == 1:
                    resized_np = resized_np.mean(axis=2, keepdims=True)  # è½¬ä¸ºç°åº¦
            
            resized_tensor = torch.from_numpy(resized_np).to(device=device, dtype=dtype)
            scaled_images.append(resized_tensor)
        
        return torch.stack(scaled_images)
    
    def _pad_image(self, image, target_width, target_height):
        """å¡«å……å›¾åƒåˆ°ç›®æ ‡å°ºå¯¸"""
        
        batch_size, orig_height, orig_width, channels = image.shape
        
        if orig_width == target_width and orig_height == target_height:
            return image
        
        device = image.device
        dtype = image.dtype
        
        # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„é»‘è‰²å›¾åƒ
        padded = torch.zeros((batch_size, target_height, target_width, channels), 
                           device=device, dtype=dtype)
        
        # è®¡ç®—å±…ä¸­ä½ç½®å’Œè£å‰ª/å¡«å……å‚æ•°
        if orig_width <= target_width and orig_height <= target_height:
            # åŸå›¾å°äºç›®æ ‡å°ºå¯¸ï¼Œéœ€è¦å¡«å……
            start_x = (target_width - orig_width) // 2
            start_y = (target_height - orig_height) // 2
            
            # å¤åˆ¶æ•´ä¸ªåŸå›¾åˆ°ç›®æ ‡ä½ç½®
            padded[:, start_y:start_y+orig_height, start_x:start_x+orig_width, :] = image
        else:
            # åŸå›¾å¤§äºç›®æ ‡å°ºå¯¸ï¼Œéœ€è¦å±…ä¸­è£å‰ªç„¶åæ”¾ç½®
            # è®¡ç®—åŸå›¾çš„è£å‰ªåŒºåŸŸ
            crop_start_x = max(0, (orig_width - target_width) // 2)
            crop_start_y = max(0, (orig_height - target_height) // 2)
            
            # è®¡ç®—å®é™…å¤åˆ¶çš„å°ºå¯¸
            copy_width = min(orig_width, target_width)
            copy_height = min(orig_height, target_height)
            
            # è®¡ç®—åœ¨ç›®æ ‡å›¾åƒä¸­çš„æ”¾ç½®ä½ç½®
            pad_start_x = max(0, (target_width - copy_width) // 2)
            pad_start_y = max(0, (target_height - copy_height) // 2)
            
            # ä»åŸå›¾è£å‰ªå¹¶å¤åˆ¶åˆ°ç›®æ ‡ä½ç½®
            padded[:, pad_start_y:pad_start_y+copy_height, pad_start_x:pad_start_x+copy_width, :] = \
                image[:, crop_start_y:crop_start_y+copy_height, crop_start_x:crop_start_x+copy_width, :]
        
        return padded
    
    def _center_crop_image(self, image, target_width, target_height):
        """å±…ä¸­è£å‰ªå›¾åƒ"""
        
        batch_size, orig_height, orig_width, channels = image.shape
        
        if orig_width == target_width and orig_height == target_height:
            return image
        
        # è®¡ç®—è£å‰ªèµ·å§‹ä½ç½®
        start_x = (orig_width - target_width) // 2
        start_y = (orig_height - target_height) // 2
        
        # ç¡®ä¿ä¸ä¼šè¶Šç•Œ
        start_x = max(0, start_x)
        start_y = max(0, start_y)
        end_x = min(start_x + target_width, orig_width)
        end_y = min(start_y + target_height, orig_height)
        
        # è£å‰ªå›¾åƒ
        cropped = image[:, start_y:end_y, start_x:end_x, :]
        
        # å¦‚æœè£å‰ªåå°ºå¯¸ä¸è¶³ï¼Œéœ€è¦å¡«å……
        actual_height, actual_width = cropped.shape[1], cropped.shape[2]
        if actual_width < target_width or actual_height < target_height:
            cropped = self._pad_image(cropped, target_width, target_height)
        
        return cropped
    
    def _create_empty_image(self, width, height):
        """åˆ›å»ºç©ºçš„é»‘è‰²å›¾åƒ"""
        return torch.zeros((1, height, width, 3), dtype=torch.float32)
    
    def _create_white_mask(self, width, height):
        """åˆ›å»ºç™½è‰²é®ç½©"""
        return torch.ones((1, height, width), dtype=torch.float32)


class MaskScale:
    """
    é®ç½©ç¼©æ”¾èŠ‚ç‚¹ - ä¸“é—¨å¤„ç†é®ç½©çš„ç¼©æ”¾åŠŸèƒ½
    
    åŠŸèƒ½:
    - ä¸“æ³¨äºé®ç½©çš„é«˜è´¨é‡ç¼©æ”¾
    - æ”¯æŒå¤šç§ç¼©æ”¾å®šä¹‰æ–¹å¼
    - æ”¯æŒä¿æŒæ¯”ä¾‹æˆ–è‡ªç”±ç¼©æ”¾
    - é«˜æ•ˆçš„æ’å€¼ç®—æ³•
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "mask": ("MASK",),
                "scale_definition": (["width", "height", "longest_side", "shortest_side", "total_pixels"], {
                    "default": "longest_side"
                }),
                "definition_value": ("INT", {
                    "default": 512,
                    "min": 1,
                    "max": 999999999,
                    "step": 1
                }),
                "interpolation": (["nearest", "bilinear", "bicubic", "lanczos"], {
                    "default": "lanczos"
                }),
                "keep_proportions": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "width": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 999999999,
                    "step": 1
                }),
                "height": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 999999999,
                    "step": 1
                }),
            }
        }
    
    # è¿”å›ç±»å‹ä¸åç§°ï¼ˆç»Ÿä¸€ä¸­æ–‡æ˜¾ç¤ºï¼‰
    RETURN_TYPES = ("MASK", "INT", "INT")
    RETURN_NAMES = ("mask", "width", "height")
    FUNCTION = "scale_mask"
    CATEGORY = "ğŸ¨QING/é®ç½©å¤„ç†"
    
    def scale_mask(self, mask, scale_definition, definition_value, interpolation, keep_proportions, width=0, height=0):
        """
        ç¼©æ”¾é®ç½©å°ºå¯¸
        - é®ç½©: è¾“å…¥é®ç½© (Tensor: [B,H,W])
        - scale_definition: ç¼©æ”¾å®šä¹‰ï¼šwidth/height/longest_side/shortest_side/total_pixels
        - definition_value: å®šä¹‰æ•°å€¼ï¼ˆä¸ scale_definition å¯¹åº”ï¼‰
        - interpolation: interpolationï¼šnearest/bilinear/bicubic/lanczos
        - keep_proportions: æ˜¯å¦ä¿æŒçºµæ¨ªæ¯”
        - width/height: æŒ‡å®šç›®æ ‡å°ºå¯¸ï¼ˆä¼˜å…ˆäº scale_definitionï¼‰
        è¿”å›ï¼šç¼©æ”¾åçš„é®ç½©ï¼Œä»¥åŠç›®æ ‡å®½é«˜
        """
        # è¾“å…¥æ ¡éªŒ
        if mask is None:
            raise ValueError("Input mask cannot be None")
        
        # Ensure input is 2D or 3D tensor
        if mask.dim() == 2:
            mask = mask.unsqueeze(0)
        elif mask.dim() != 3:
            raise ValueError(f"Mask dimension should be 2 or 3, got {mask.dim()}")
        
        # è·å–å½“å‰é®ç½©å°ºå¯¸
        batch_size, orig_height, orig_width = mask.shape
        
        # è‹¥æ˜¾å¼æä¾›ç›®æ ‡å°ºå¯¸ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨
        if width > 0 and height > 0:
            target_width = max(1, width)
            target_height = max(1, height)
        else:
            # æ ¹æ®ç¼©æ”¾æ–¹å¼è®¡ç®—ç›®æ ‡å°ºå¯¸
            orig_pixels = orig_height * orig_width
            
            if scale_definition == "width":
                target_width = definition_value
                if keep_proportions:
                    target_height = max(1, int(round(orig_height * (target_width / orig_width))))
                else:
                    target_height = orig_height
                    
            elif scale_definition == "height":
                target_height = definition_value
                if keep_proportions:
                    target_width = max(1, int(round(orig_width * (target_height / orig_height))))
                else:
                    target_width = orig_width
                    
            elif scale_definition == "longest_side":
                if orig_height >= orig_width:
                    target_height = definition_value
                    if keep_proportions:
                        target_width = max(1, int(round(orig_width * (target_height / orig_height))))
                    else:
                        target_width = orig_width
                else:
                    target_width = definition_value
                    if keep_proportions:
                        target_height = max(1, int(round(orig_height * (target_width / orig_width))))
                    else:
                        target_height = orig_height
                        
            elif scale_definition == "shortest_side":
                if orig_height <= orig_width:
                    target_height = definition_value
                    if keep_proportions:
                        target_width = max(1, int(round(orig_width * (target_height / orig_height))))
                    else:
                        target_width = orig_width
                else:
                    target_width = definition_value
                    if keep_proportions:
                        target_height = max(1, int(round(orig_height * (target_width / orig_width))))
                    else:
                        target_height = orig_height
                        
            elif scale_definition == "total_pixels":
                # ä¾æ®æ€»åƒç´ è®¡ç®—ç¼©æ”¾å› å­
                scale_factor = (definition_value / orig_pixels) ** 0.5
                target_width = max(1, int(round(orig_width * scale_factor)))
                target_height = max(1, int(round(orig_height * scale_factor)))
                
                # è‹¥keep_proportionsï¼Œç»†è°ƒä»¥æ›´æ¥è¿‘ç›®æ ‡åƒç´ 
                if keep_proportions:
                    actual_pixels = target_width * target_height
                    if abs(actual_pixels - definition_value) / definition_value > 0.1:
                        alternative_width = max(1, int(round((definition_value * orig_width / orig_height) ** 0.5)))
                        alternative_height = max(1, int(round(definition_value / alternative_width)))
                        if abs(alternative_width * alternative_height - definition_value) < abs(actual_pixels - definition_value):
                            target_width, target_height = alternative_width, alternative_height
        
        # ä¿è¯ç›®æ ‡å°ºå¯¸è‡³å°‘ä¸º 1x1
        target_width = max(1, target_width)
        target_height = max(1, target_height)
        
        # è®°å½•åŸå§‹è®¾å¤‡ä¸æ•°æ®ç±»å‹
        device = mask.device
        dtype = mask.dtype
        
        # å½“é€‰æ‹© Lanczos æ—¶ï¼Œä½¿ç”¨ PIL è·å¾—æ›´é«˜è´¨é‡
        if interpolation == "lanczos":
            scaled_masks = []
            for i in range(batch_size):
                # è½¬ä¸º PIL å›¾åƒ
                mask_np = mask[i].cpu().numpy() * 255
                mask_pil = Image.fromarray(mask_np.astype(np.uint8), mode='L')
                
                # Lanczos æ’å€¼ç¼©æ”¾
                mask_pil = mask_pil.resize((target_width, target_height), Image.LANCZOS)
                
                # è½¬å›å¼ é‡
                mask_np = np.array(mask_pil).astype(np.float32) / 255.0
                mask_tensor = torch.from_numpy(mask_np).to(device=device, dtype=dtype)
                scaled_masks.append(mask_tensor)
            
            scaled_mask = torch.stack(scaled_masks)
        else:
            # é€‰æ‹© PyTorch çš„æ’å€¼æ¨¡å¼
            if interpolation == "nearest":
                mode = "nearest"
            elif interpolation == "bilinear":
                mode = "bilinear"
            else:  # bicubic
                mode = "bicubic"
            
            # è½¬ä¸º float ä»¥ä¾¿æ’å€¼
            mask_float = mask.float()
            
            # è¿›è¡Œå°ºå¯¸å˜æ¢
            scaled_mask = torch.nn.functional.interpolate(
                mask_float.unsqueeze(1),
                size=(target_height, target_width),
                mode=mode,
                align_corners=False if mode != "nearest" else None
            ).squeeze(1)
            
            # è½¬å›åŸ dtype
            if dtype != torch.float32:
                scaled_mask = scaled_mask.to(dtype)
        
        # ä¿è¯æ•°å€¼åœ¨ 0-1 èŒƒå›´
        scaled_mask = torch.clamp(scaled_mask, 0.0, 1.0)
        
        return (scaled_mask, target_width, target_height)


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "Imagesizescaling": Imagesizescaling,
    "MaskScale": MaskScale
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Imagesizescaling": "å›¾åƒç¼©æ”¾",
    "MaskScale": "é®ç½©ç¼©æ”¾"
}
